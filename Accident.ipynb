{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOL9A1JlT2Hp",
        "outputId": "ea96852e-7276-485c-b57f-9c00d3d4399b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "TensorFlow 2.19.0\n",
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Cell 0 — GPU check & mount Drive\n",
        "import sys, os\n",
        "import tensorflow as tf\n",
        "print(\"Python\", sys.version)\n",
        "print(\"TensorFlow\", tf.__version__)\n",
        "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Mount Drive (will prompt for OAuth)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — Config & install deps\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/AccidentProject\"\n",
        "DATA_RAW = os.path.join(PROJECT_DIR, \"data\", \"raw\")\n",
        "FEATURE_DIR = os.path.join(PROJECT_DIR, \"data\", \"features\")\n",
        "CHECKPOINT_DIR = os.path.join(PROJECT_DIR, \"checkpoints\")\n",
        "LOG_DIR = os.path.join(PROJECT_DIR, \"logs\")\n",
        "EXPORT_DIR = os.path.join(PROJECT_DIR, \"exports\", \"models\")\n",
        "os.makedirs(DATA_RAW, exist_ok=True)\n",
        "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "# Install packages if not present\n",
        "!pip install -q tensorflow-addons opencv-python-headless tqdm scikit-learn\n",
        "\n",
        "# Enable mixed precision for speed (optional but recommended)\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"Mixed precision policy:\", mixed_precision.global_policy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBHCB7wOT8dD",
        "outputId": "ef20fb88-bcf8-49d5-fb2a-31a421d518f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-addons\u001b[0m\u001b[31m\n",
            "\u001b[0mMixed precision policy: <DTypePolicy \"mixed_float16\">\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 — imports and small helpers\n",
        "import numpy as np, cv2, glob, json, math, random, time\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "print(\"Imports ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYlYj3qET90y",
        "outputId": "8e538daf-07b5-4800-ebdc-88139ab6ec60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 — timestamp parsing and label loader (expects HH:MM:SS - HH:MM:SS per line)\n",
        "def to_seconds(hms):\n",
        "    hms = str(hms).strip()\n",
        "    parts = [int(p) for p in hms.split(\":\")]\n",
        "    if len(parts) == 3:\n",
        "        h,m,s = parts\n",
        "    elif len(parts) == 2:\n",
        "        h = 0; m,s = parts\n",
        "    else:\n",
        "        raise ValueError(\"Bad timestamp: \" + hms)\n",
        "    return h*3600 + m*60 + s\n",
        "\n",
        "def load_intervals(txt_path):\n",
        "    intervals = []\n",
        "    if not os.path.exists(txt_path):\n",
        "        return intervals\n",
        "    for line in open(txt_path, 'r'):\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "        # handle \"HH:MM:SS - HH:MM:SS\" or \"HH:MM:SS to HH:MM:SS\"\n",
        "        if \"-\" in line:\n",
        "            a,b = line.split(\"-\")\n",
        "        elif \"to\" in line:\n",
        "            a,b = line.split(\"to\")\n",
        "        else:\n",
        "            continue\n",
        "        try:\n",
        "            intervals.append((to_seconds(a), to_seconds(b)))\n",
        "        except:\n",
        "            continue\n",
        "    return intervals\n",
        "\n",
        "def window_overlaps(start_t, end_t, intervals):\n",
        "    for a,b in intervals:\n",
        "        if start_t <= b and end_t >= a:\n",
        "            return True\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "ONr_9S88UAqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 — CCTV-safe augmentations without imgaug (use this instead)\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "def adjust_brightness(frame, factor):\n",
        "    # factor between 0.8 and 1.2\n",
        "    return np.clip(frame * factor, 0, 255).astype(np.uint8)\n",
        "\n",
        "def adjust_contrast(frame, factor):\n",
        "    # simple contrast adjustment\n",
        "    mean = frame.mean(axis=(0,1), keepdims=True)\n",
        "    return np.clip((frame - mean) * factor + mean, 0, 255).astype(np.uint8)\n",
        "\n",
        "def gaussian_noise(frame, std=10):\n",
        "    noise = np.random.normal(0, std, frame.shape)\n",
        "    out = frame + noise\n",
        "    return np.clip(out, 0, 255).astype(np.uint8)\n",
        "\n",
        "def jpeg_compress(frame, quality=60):\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
        "    _, enc = cv2.imencode('.jpg', frame[:,:,::-1], encode_param)\n",
        "    dec = cv2.imdecode(enc, cv2.IMREAD_COLOR)[:,:,::-1]\n",
        "    return dec\n",
        "\n",
        "def motion_blur(frame, ksize=3):\n",
        "    kernel = np.zeros((ksize, ksize))\n",
        "    kernel[int((ksize-1)/2), :] = 1.0 / ksize\n",
        "    return cv2.filter2D(frame, -1, kernel)\n",
        "\n",
        "def augment_frame(frame, apply_prob=0.6):\n",
        "    if random.random() > apply_prob:\n",
        "        return frame\n",
        "\n",
        "    # Apply 2–4 random lightweight augmentations\n",
        "    ops = []\n",
        "    if random.random() < 0.7: ops.append(\"brightness\")\n",
        "    if random.random() < 0.7: ops.append(\"contrast\")\n",
        "    if random.random() < 0.4: ops.append(\"noise\")\n",
        "    if random.random() < 0.4: ops.append(\"blur\")\n",
        "    if random.random() < 0.6: ops.append(\"jpeg\")\n",
        "\n",
        "    random.shuffle(ops)\n",
        "\n",
        "    f = frame.copy()\n",
        "    for op in ops:\n",
        "        if op == \"brightness\":\n",
        "            f = adjust_brightness(f, random.uniform(0.8, 1.2))\n",
        "        elif op == \"contrast\":\n",
        "            f = adjust_contrast(f, random.uniform(0.8, 1.25))\n",
        "        elif op == \"noise\":\n",
        "            f = gaussian_noise(f, std=random.uniform(3, 12))\n",
        "        elif op == \"blur\":\n",
        "            f = motion_blur(f, ksize=random.choice([3,5]))\n",
        "        elif op == \"jpeg\":\n",
        "            f = jpeg_compress(f, quality=random.randint(40, 90))\n",
        "\n",
        "    return f\n"
      ],
      "metadata": {
        "id": "-HkfQJUlUBfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 — Feature extraction (EfficientNet-B1)\n",
        "from tensorflow.keras.applications import EfficientNetB1\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "IMG_H = 224; IMG_W = 224\n",
        "BACKBONE = EfficientNetB1(weights=\"imagenet\", include_top=False, pooling='avg', input_shape=(IMG_H, IMG_W, 3))\n",
        "BACKBONE.trainable = False\n",
        "print(\"Backbone ready:\", BACKBONE.name)\n",
        "\n",
        "def extract_features_for_video(video_path, out_feat_path, target_fps=16, augment=False, overwrite=False):\n",
        "    if os.path.exists(out_feat_path) and not overwrite:\n",
        "        print(\"Skipping (exists):\", out_feat_path)\n",
        "        return\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(\"Can't open video: \" + video_path)\n",
        "    orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    step = max(1, int(round(orig_fps / target_fps)))\n",
        "    frames = []\n",
        "    idx = 0\n",
        "    while True:\n",
        "        ret, f = cap.read()\n",
        "        if not ret: break\n",
        "        if idx % step == 0:\n",
        "            f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
        "            f = cv2.resize(f, (IMG_W, IMG_H))\n",
        "            if augment:\n",
        "                f = augment_frame(f)\n",
        "            frames.append(f)\n",
        "        idx += 1\n",
        "    cap.release()\n",
        "    if len(frames) == 0:\n",
        "        np.save(out_feat_path, np.zeros((0, BACKBONE.output_shape[-1]), dtype=np.float32))\n",
        "        return\n",
        "    batch = np.array(frames, dtype=np.float32)\n",
        "    batch = preprocess_input(batch)\n",
        "    # run in batches if many frames\n",
        "    feat_list = []\n",
        "    B = 64\n",
        "    for i in range(0, len(batch), B):\n",
        "        sub = batch[i:i+B]\n",
        "        feats = BACKBONE.predict(sub, verbose=0)\n",
        "        feat_list.append(feats)\n",
        "    feats_all = np.concatenate(feat_list, axis=0)\n",
        "    np.save(out_feat_path, feats_all.astype(np.float32))\n",
        "    print(f\"Saved features {out_feat_path} shape={feats_all.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv6K0IS_UD-1",
        "outputId": "26c3fa73-6f67-4388-a25d-da9bba36c690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
            "\u001b[1m27018416/27018416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Backbone ready: efficientnetb1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 — extract features for every video in data/raw\n",
        "video_files = sorted([p for p in glob.glob(os.path.join(DATA_RAW, \"*\")) if p.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))])\n",
        "print(\"Found\", len(video_files), \"videos in\", DATA_RAW)\n",
        "\n",
        "for v in tqdm(video_files):\n",
        "    base = Path(v).stem\n",
        "    out = os.path.join(FEATURE_DIR, base + \"-feat.npy\")\n",
        "    extract_features_for_video(v, out, target_fps=16, augment=False, overwrite=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1Z5Fj8RUGoB",
        "outputId": "7209c2f9-4bb7-430d-e6ce-2cb51e2a0bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14 videos in /content/drive/MyDrive/AccidentProject/data/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 2907.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/1XIS9UOwC_Y-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/3KSHg1inZS8-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/3LsNpm5y-VA-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/AuQz_-J2kzc-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/Copy of BpMHNEa79lo-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/Copy of L334aqEJxys-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/Copy of N6zJ2inqJn0-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/Copy of YzocCFJIbCc-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/Copy of bCr5Grrfw4I-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/Copy of hZm7dC8qBO0-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/Copy of mhQIf0yVU3g-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/Copy of xKT7khciy-c-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/HueSPfF0dSQ-feat.npy\n",
            "Skipping (exists): /content/drive/MyDrive/AccidentProject/data/features/bSZkOI7eF8k-feat.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 — Build window index files (per-video index list)\n",
        "SEQ_LEN = 48\n",
        "STRIDE = 8  # you can tune stride; smaller -> more samples\n",
        "\n",
        "index_entries = []  # list of (feat_path, start_frame_idx, label)\n",
        "\n",
        "for feat_path in tqdm(sorted(glob.glob(os.path.join(FEATURE_DIR, \"*-feat.npy\")))):\n",
        "    base = Path(feat_path).stem.replace(\"-feat\", \"\")\n",
        "    # find corresponding txt in data/raw\n",
        "    txt_path = None\n",
        "    for ext in ('.txt', '.label'):\n",
        "        cand = os.path.join(DATA_RAW, base + ext)\n",
        "        if os.path.exists(cand): txt_path = cand; break\n",
        "    intervals = load_intervals(txt_path) if txt_path else []\n",
        "    feats = np.load(feat_path, mmap_mode='r')\n",
        "    n_frames = feats.shape[0]\n",
        "    if n_frames < SEQ_LEN:\n",
        "        continue\n",
        "    i = 0\n",
        "    while i + SEQ_LEN <= n_frames:\n",
        "        s_t = i / 16.0\n",
        "        e_t = (i + SEQ_LEN - 1) / 16.0\n",
        "        label = 1 if window_overlaps(s_t, e_t, intervals) else 0\n",
        "        index_entries.append((feat_path, i, label))\n",
        "        i += STRIDE\n",
        "\n",
        "# Save index to disk (so you can reload fast)\n",
        "index_file = os.path.join(PROJECT_DIR, \"dataset_index.npy\")\n",
        "np.save(index_file, np.array(index_entries, dtype=object))\n",
        "print(\"Total windows:\", len(index_entries))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX6nmcyQUIcY",
        "outputId": "3846ae8c-012f-4780-ffd7-5852a1a51f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:28<00:00,  2.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total windows: 12872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 — feature-level dataset generator (yields batches of shape [B, SEQ_LEN, feat_dim])\n",
        "from sklearn.utils import resample\n",
        "def feature_window_generator(index_file, batch_size=32, shuffle=True, balance=True):\n",
        "    entries = list(np.load(index_file, allow_pickle=True))\n",
        "    idxs = list(range(len(entries)))\n",
        "    while True:\n",
        "        if shuffle: random.shuffle(idxs)\n",
        "        Xb, yb = [], []\n",
        "        for j in idxs:\n",
        "            feat_path, start_idx, label = entries[j]\n",
        "            feats = np.load(feat_path, mmap_mode='r')  # memory efficient\n",
        "            clip = feats[start_idx:start_idx+SEQ_LEN]  # shape (SEQ_LEN, feat_dim)\n",
        "            if clip.shape[0] != SEQ_LEN:\n",
        "                continue\n",
        "            Xb.append(clip)\n",
        "            yb.append(label)\n",
        "            if len(Xb) >= batch_size:\n",
        "                X = np.array(Xb, dtype=np.float32)\n",
        "                y = np.array(yb, dtype=np.float32)\n",
        "                if balance:\n",
        "                    # simple on-batch balancing: oversample positives if needed\n",
        "                    pos_idx = np.where(y==1)[0]\n",
        "                    neg_idx = np.where(y==0)[0]\n",
        "                    if len(pos_idx)>0 and len(pos_idx) < len(neg_idx):\n",
        "                        extra = np.random.choice(pos_idx, size=(len(neg_idx)-len(pos_idx)), replace=True)\n",
        "                        X = np.concatenate([X, X[extra]], axis=0)\n",
        "                        y = np.concatenate([y, y[extra]], axis=0)\n",
        "                perm = np.random.permutation(len(X))\n",
        "                yield X[perm], y[perm]\n",
        "                Xb, yb = [], []\n",
        "        # flush remaining\n",
        "        if len(Xb) > 0:\n",
        "            X = np.array(Xb, dtype=np.float32)\n",
        "            y = np.array(yb, dtype=np.float32)\n",
        "            if balance:\n",
        "                pos_idx = np.where(y==1)[0]\n",
        "                neg_idx = np.where(y==0)[0]\n",
        "                if len(pos_idx)>0 and len(pos_idx) < len(neg_idx):\n",
        "                    extra = np.random.choice(pos_idx, size=(len(neg_idx)-len(pos_idx)), replace=True)\n",
        "                    X = np.concatenate([X, X[extra]], axis=0)\n",
        "                    y = np.concatenate([y, y[extra]], axis=0)\n",
        "            perm = np.random.permutation(len(X))\n",
        "            yield X[perm], y[perm]\n"
      ],
      "metadata": {
        "id": "IZQ_rLSrUJBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 — Transformer encoder on top of features\n",
        "import math\n",
        "def build_transformer_temporal(seq_len=SEQ_LEN, feat_dim=1280, d_model=512, n_layers=3, n_heads=8, ffn_dim=2048, dropout=0.1):\n",
        "    inp = layers.Input(shape=(seq_len, feat_dim), name=\"feat_input\")\n",
        "    # Project features to d_model\n",
        "    x = layers.Dense(d_model, activation=None, name=\"proj\")(inp)\n",
        "    x = layers.LayerNormalization(name=\"ln_in\")(x)\n",
        "    # Positional embeddings (learned)\n",
        "    pos = layers.Embedding(input_dim=seq_len, output_dim=d_model)(tf.range(start=0, limit=seq_len, delta=1))\n",
        "    x = x + pos\n",
        "    # Transformer stacks (Pre-LN pattern)\n",
        "    for i in range(n_layers):\n",
        "        # Multi-head self-attention\n",
        "        x1 = layers.LayerNormalization(name=f\"ln_att_{i}\")(x)\n",
        "        att = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model//n_heads, dropout=dropout, name=f\"mha_{i}\")(x1, x1)\n",
        "        x = layers.Add()([x, att])\n",
        "        # FFN\n",
        "        x2 = layers.LayerNormalization(name=f\"ln_ffn_{i}\")(x)\n",
        "        f = layers.Dense(ffn_dim, activation='relu')(x2)\n",
        "        f = layers.Dropout(dropout)(f)\n",
        "        f = layers.Dense(d_model)(f)\n",
        "        x = layers.Add()([x, f])\n",
        "    # Pooling & head\n",
        "    x = layers.LayerNormalization(name=\"ln_out\")(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    # final output float32 to avoid FP16 loss issues\n",
        "    out = layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
        "    model = keras.Model(inputs=inp, outputs=out, name=\"TemporalTransformer\")\n",
        "    return model\n",
        "\n",
        "# quick build\n",
        "feat_sample = np.load(sorted(glob.glob(os.path.join(FEATURE_DIR, \"*-feat.npy\")))[0])\n",
        "feat_dim = feat_sample.shape[1]\n",
        "model = build_transformer_temporal(seq_len=SEQ_LEN, feat_dim=feat_dim, d_model=512, n_layers=3, n_heads=8, ffn_dim=2048)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EVsrozbXUK26",
        "outputId": "d26cb10d-d1e2-4c7c-f52a-f24a65a54a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"TemporalTransformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TemporalTransformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ feat_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1280\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ proj (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m655,872\u001b[0m │ feat_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_in               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ proj[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ ln_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_0            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_0               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_att_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ ln_att_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                     │                   │            │ mha_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_0            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_ffn_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,049,088\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_att_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ ln_att_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ mha_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_ffn_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,049,088\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_2               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_att_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ ln_att_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ mha_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_2            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │  \u001b[38;5;34m1,050,624\u001b[0m │ ln_ffn_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,049,088\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_out              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ ln_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m65,664\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ feat_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ proj (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │ feat_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_in               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ proj[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ln_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_0            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_0               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_att_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ ln_att_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                     │                   │            │ mha_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_0            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_ffn_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_att_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ ln_att_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ mha_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_ffn_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_att_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mha_2               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_att_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ ln_att_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ mha_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_ffn_2            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ ln_ffn_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ln_out              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ln_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,180,865\u001b[0m (38.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,180,865</span> (38.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,180,865\u001b[0m (38.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,180,865</span> (38.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 — training utilities & compile (NO TFA NEEDED)\n",
        "\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "\n",
        "LR = 1e-4\n",
        "opt = AdamW(learning_rate=LR, weight_decay=1e-4)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "\n",
        "ckpt_path = os.path.join(CHECKPOINT_DIR, \"transformer_best.h5\")\n",
        "\n",
        "cb_list = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        ckpt_path,\n",
        "        monitor='val_auc',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_auc',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        patience=6,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n"
      ],
      "metadata": {
        "id": "XRDKD-ApUMWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 — Stage 1 training (transformer head only)\n",
        "BATCH_SIZE = 32\n",
        "STEPS_PER_EPOCH = max(10,  len(np.load(index_file, allow_pickle=True)) // BATCH_SIZE )\n",
        "VAL_SPLIT = 0.1\n",
        "\n",
        "# create generator\n",
        "gen = feature_window_generator(index_file, batch_size=BATCH_SIZE, shuffle=True, balance=True)\n",
        "\n",
        "# If you have a separate val index, use it. Here we do a quick random val split by entries.\n",
        "entries = list(np.load(index_file, allow_pickle=True))\n",
        "random.shuffle(entries)\n",
        "n_val = int(len(entries)*VAL_SPLIT)\n",
        "val_entries = entries[:n_val]\n",
        "train_entries = entries[n_val:]\n",
        "tmp_idx_file = os.path.join(PROJECT_DIR, \"tmp_train_idx.npy\")\n",
        "tmp_val_idx_file = os.path.join(PROJECT_DIR, \"tmp_val_idx.npy\")\n",
        "np.save(tmp_idx_file, np.array(train_entries, dtype=object))\n",
        "np.save(tmp_val_idx_file, np.array(val_entries, dtype=object))\n",
        "\n",
        "train_gen = feature_window_generator(tmp_idx_file, batch_size=BATCH_SIZE, shuffle=True, balance=True)\n",
        "val_gen = feature_window_generator(tmp_val_idx_file, batch_size=BATCH_SIZE, shuffle=False, balance=False)\n",
        "\n",
        "EPOCHS_STAGE1 = 30\n",
        "history = model.fit(train_gen,\n",
        "                    steps_per_epoch=max(10, len(train_entries)//BATCH_SIZE),\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=max(1, len(val_entries)//BATCH_SIZE),\n",
        "                    epochs=EPOCHS_STAGE1,\n",
        "                    callbacks=cb_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyjJ2agmUN5G",
        "outputId": "174b9086-1409-410f-82c9-d7e86b58967c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699ms/step - accuracy: 0.6643 - auc: 0.7271 - loss: 0.6213\n",
            "Epoch 1: val_auc improved from -inf to 0.86350, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 774ms/step - accuracy: 0.6644 - auc: 0.7273 - loss: 0.6212 - val_accuracy: 0.7773 - val_auc: 0.8635 - val_loss: 0.4372 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.7839 - auc: 0.8555 - loss: 0.4567\n",
            "Epoch 2: val_auc improved from 0.86350 to 0.91310, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 239ms/step - accuracy: 0.7840 - auc: 0.8556 - loss: 0.4566 - val_accuracy: 0.8359 - val_auc: 0.9131 - val_loss: 0.3351 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.8285 - auc: 0.9049 - loss: 0.3753\n",
            "Epoch 3: val_auc improved from 0.91310 to 0.93101, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 299ms/step - accuracy: 0.8286 - auc: 0.9049 - loss: 0.3752 - val_accuracy: 0.8242 - val_auc: 0.9310 - val_loss: 0.3675 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8693 - auc: 0.9331 - loss: 0.3141\n",
            "Epoch 4: val_auc improved from 0.93101 to 0.93784, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 267ms/step - accuracy: 0.8693 - auc: 0.9331 - loss: 0.3141 - val_accuracy: 0.8677 - val_auc: 0.9378 - val_loss: 0.2884 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8955 - auc: 0.9548 - loss: 0.2534\n",
            "Epoch 5: val_auc improved from 0.93784 to 0.95547, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 257ms/step - accuracy: 0.8955 - auc: 0.9548 - loss: 0.2534 - val_accuracy: 0.8797 - val_auc: 0.9555 - val_loss: 0.2594 - learning_rate: 5.0000e-05\n",
            "Epoch 6/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9057 - auc: 0.9647 - loss: 0.2227\n",
            "Epoch 6: val_auc improved from 0.95547 to 0.96049, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 261ms/step - accuracy: 0.9057 - auc: 0.9647 - loss: 0.2227 - val_accuracy: 0.9028 - val_auc: 0.9605 - val_loss: 0.2392 - learning_rate: 5.0000e-05\n",
            "Epoch 7/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9204 - auc: 0.9743 - loss: 0.1898\n",
            "Epoch 7: val_auc improved from 0.96049 to 0.96787, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 259ms/step - accuracy: 0.9204 - auc: 0.9743 - loss: 0.1898 - val_accuracy: 0.9060 - val_auc: 0.9679 - val_loss: 0.2303 - learning_rate: 5.0000e-05\n",
            "Epoch 8/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9363 - auc: 0.9837 - loss: 0.1519\n",
            "Epoch 8: val_auc improved from 0.96787 to 0.97234, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 262ms/step - accuracy: 0.9363 - auc: 0.9837 - loss: 0.1519 - val_accuracy: 0.9163 - val_auc: 0.9723 - val_loss: 0.1987 - learning_rate: 2.5000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9503 - auc: 0.9880 - loss: 0.1267\n",
            "Epoch 9: val_auc improved from 0.97234 to 0.97367, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 259ms/step - accuracy: 0.9503 - auc: 0.9880 - loss: 0.1267 - val_accuracy: 0.9076 - val_auc: 0.9737 - val_loss: 0.2100 - learning_rate: 2.5000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9483 - auc: 0.9889 - loss: 0.1238\n",
            "Epoch 10: val_auc improved from 0.97367 to 0.97687, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 260ms/step - accuracy: 0.9483 - auc: 0.9889 - loss: 0.1238 - val_accuracy: 0.9171 - val_auc: 0.9769 - val_loss: 0.1935 - learning_rate: 2.5000e-05\n",
            "Epoch 11/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9577 - auc: 0.9914 - loss: 0.1069\n",
            "Epoch 11: val_auc did not improve from 0.97687\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 248ms/step - accuracy: 0.9577 - auc: 0.9914 - loss: 0.1068 - val_accuracy: 0.9203 - val_auc: 0.9716 - val_loss: 0.2194 - learning_rate: 1.2500e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9661 - auc: 0.9939 - loss: 0.0892\n",
            "Epoch 12: val_auc did not improve from 0.97687\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 239ms/step - accuracy: 0.9661 - auc: 0.9939 - loss: 0.0892 - val_accuracy: 0.9187 - val_auc: 0.9724 - val_loss: 0.2117 - learning_rate: 1.2500e-05\n",
            "Epoch 13/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9632 - auc: 0.9946 - loss: 0.0873\n",
            "Epoch 13: val_auc did not improve from 0.97687\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 243ms/step - accuracy: 0.9632 - auc: 0.9946 - loss: 0.0873 - val_accuracy: 0.9147 - val_auc: 0.9732 - val_loss: 0.2242 - learning_rate: 1.2500e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9701 - auc: 0.9951 - loss: 0.0790\n",
            "Epoch 14: val_auc improved from 0.97687 to 0.97796, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 254ms/step - accuracy: 0.9701 - auc: 0.9951 - loss: 0.0790 - val_accuracy: 0.9227 - val_auc: 0.9780 - val_loss: 0.2022 - learning_rate: 6.2500e-06\n",
            "Epoch 15/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9755 - auc: 0.9965 - loss: 0.0660\n",
            "Epoch 15: val_auc did not improve from 0.97796\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 248ms/step - accuracy: 0.9755 - auc: 0.9965 - loss: 0.0660 - val_accuracy: 0.9267 - val_auc: 0.9752 - val_loss: 0.2143 - learning_rate: 6.2500e-06\n",
            "Epoch 16/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9762 - auc: 0.9969 - loss: 0.0634\n",
            "Epoch 16: val_auc improved from 0.97796 to 0.97911, saving model to /content/drive/MyDrive/AccidentProject/checkpoints/transformer_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 251ms/step - accuracy: 0.9762 - auc: 0.9969 - loss: 0.0634 - val_accuracy: 0.9267 - val_auc: 0.9791 - val_loss: 0.1939 - learning_rate: 6.2500e-06\n",
            "Epoch 17/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9781 - auc: 0.9971 - loss: 0.0608\n",
            "Epoch 17: val_auc did not improve from 0.97911\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 248ms/step - accuracy: 0.9781 - auc: 0.9971 - loss: 0.0608 - val_accuracy: 0.9307 - val_auc: 0.9761 - val_loss: 0.2218 - learning_rate: 3.1250e-06\n",
            "Epoch 18/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9766 - auc: 0.9974 - loss: 0.0589\n",
            "Epoch 18: val_auc did not improve from 0.97911\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 244ms/step - accuracy: 0.9766 - auc: 0.9974 - loss: 0.0589 - val_accuracy: 0.9275 - val_auc: 0.9779 - val_loss: 0.2130 - learning_rate: 3.1250e-06\n",
            "Epoch 19/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9801 - auc: 0.9979 - loss: 0.0518\n",
            "Epoch 19: val_auc did not improve from 0.97911\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 244ms/step - accuracy: 0.9801 - auc: 0.9979 - loss: 0.0518 - val_accuracy: 0.9227 - val_auc: 0.9769 - val_loss: 0.2392 - learning_rate: 3.1250e-06\n",
            "Epoch 20/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9800 - auc: 0.9973 - loss: 0.0544\n",
            "Epoch 20: val_auc did not improve from 0.97911\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 241ms/step - accuracy: 0.9800 - auc: 0.9973 - loss: 0.0544 - val_accuracy: 0.9283 - val_auc: 0.9759 - val_loss: 0.2293 - learning_rate: 1.5625e-06\n",
            "Epoch 21/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9819 - auc: 0.9983 - loss: 0.0474\n",
            "Epoch 21: val_auc did not improve from 0.97911\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 244ms/step - accuracy: 0.9819 - auc: 0.9983 - loss: 0.0474 - val_accuracy: 0.9299 - val_auc: 0.9726 - val_loss: 0.2394 - learning_rate: 1.5625e-06\n",
            "Epoch 22/30\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9822 - auc: 0.9982 - loss: 0.0462\n",
            "Epoch 22: val_auc did not improve from 0.97911\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
            "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 243ms/step - accuracy: 0.9822 - auc: 0.9982 - loss: 0.0462 - val_accuracy: 0.9267 - val_auc: 0.9719 - val_loss: 0.2454 - learning_rate: 1.5625e-06\n",
            "Epoch 22: early stopping\n",
            "Restoring model weights from the end of the best epoch: 16.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 — Optional: fine-tune some backbone layers\n",
        "# Only do this if Stage1 is good and you have GPU and patience.\n",
        "FINETUNE = True\n",
        "if FINETUNE:\n",
        "    # load best transformer weights\n",
        "    model.load_weights(ckpt_path)\n",
        "    # Rebuild a model that includes backbone if you want end-to-end; or fine-tune backbone features by re-extracting features with trainable backbone (complex).\n",
        "    # Simpler approach: fine-tune EfficientNet on raw frames with a small classifier and small LR,\n",
        "    # or re-extract features using a slightly fine-tuned backbone. This step is dataset-dependent and more complex.\n",
        "    print(\"Stage 2 suggested approach: re-train backbone on frames using smaller LR or fine-tune last layers offline.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyWuK_MSUQhH",
        "outputId": "885be564-35f3-4e61-ae0a-1847e99e9fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 suggested approach: re-train backbone on frames using smaller LR or fine-tune last layers offline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 — evaluation helpers\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
        "\n",
        "def evaluate_model_on_index(model, index_entries, batch_size=32):\n",
        "    y_true = []\n",
        "    y_prob = []\n",
        "    for i in range(0, len(index_entries), batch_size):\n",
        "        batch = index_entries[i:i+batch_size]\n",
        "        X = []\n",
        "        for feat_path, start, lbl in batch:\n",
        "            f = np.load(feat_path, mmap_mode='r')[start:start+SEQ_LEN]\n",
        "            X.append(f)\n",
        "            y_true.append(lbl)\n",
        "        X = np.array(X, dtype=np.float32)\n",
        "        preds = model.predict(X, verbose=0).ravel()\n",
        "        y_prob.extend(preds.tolist())\n",
        "    y_true = np.array(y_true)\n",
        "    y_prob = np.array(y_prob)\n",
        "    auc = roc_auc_score(y_true, y_prob) if y_true.sum()>0 else None\n",
        "    ap = average_precision_score(y_true, y_prob) if y_true.sum()>0 else None\n",
        "    print(\"AUC:\", auc, \"AP:\", ap)\n",
        "    return y_true, y_prob\n",
        "\n",
        "# quick evaluation on validation entries if exists\n",
        "if os.path.exists(tmp_val_idx_file):\n",
        "    val_entries = list(np.load(tmp_val_idx_file, allow_pickle=True))\n",
        "    _y, _p = evaluate_model_on_index(model, val_entries[:2000], batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9dJf4H9URKN",
        "outputId": "5168d25d-15e0-4285-d382-f5c57ff6129c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9836104029709853 AP: 0.9634113577747099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os\n",
        "video_path = \"/content/drive/MyDrive/AccidentProject/data/raw/HueSPfF0dSQ.mp4\"  # replace with the path you used\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "print(\"exists:\", os.path.exists(video_path))\n",
        "print(\"cap.isOpened():\", cap.isOpened())\n",
        "if cap.isOpened():\n",
        "    print(\"frame_count:\", cap.get(cv2.CAP_PROP_FRAME_COUNT), \"FPS:\", cap.get(cv2.CAP_PROP_FPS))\n",
        "cap.release()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqGWNCgFpXPM",
        "outputId": "2493c1f5-fb7e-4d3e-b199-4c3d3a6c0775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exists: True\n",
            "cap.isOpened(): True\n",
            "frame_count: 11988.0 FPS: 25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, cv2\n",
        "video_path = \"/content/drive/MyDrive/AccidentProject/data/raw/HueSPfF0dSQ.mp4\"  # same as above\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "t0 = time.time()\n",
        "count = 0\n",
        "max_reads = 200\n",
        "while count < max_reads:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"end of file at frame\", count); break\n",
        "    count += 1\n",
        "t1 = time.time()\n",
        "cap.release()\n",
        "print(\"Read frames:\", count, \"time:\", t1 - t0, \"s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGGIO7qIpiJ1",
        "outputId": "7207b10d-d02e-4eb2-e6d5-a3158c51dc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read frames: 200 time: 0.5152714252471924 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, time\n",
        "# build one dummy preprocessed image like your pipeline produces:\n",
        "img = np.zeros((1, IMG_H, IMG_W, 3), dtype=np.float32)  # replace IMG_* values from notebook\n",
        "t0 = time.time()\n",
        "_ = BACKBONE.predict(img, verbose=0)\n",
        "print(\"BACKBONE.predict on 1 frame took\", time.time()-t0, \"s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_srbMT8pwsr",
        "outputId": "b164b135-a95d-4d56-9230-6c0d57c198f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BACKBONE.predict on 1 frame took 28.503289461135864 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dummy feature window\n",
        "dummy = np.zeros((1, SEQ_LEN, feat_dim), dtype=np.float32)\n",
        "t0 = time.time()\n",
        "_ = model.predict(dummy, verbose=0)\n",
        "print(\"temporal model predict time:\", time.time()-t0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hg4SN70pz0t",
        "outputId": "1de012f7-58de-4b71-e917-858e76c72962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temporal model predict time: 1.109013557434082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, time, cv2, numpy as np\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "def inference_on_video_fast(video_path, model, target_fps=16, seq_len=SEQ_LEN, stride=4, threshold=0.6, log_dir=LOG_DIR, local_copy=True):\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    # 1) Copy to local /tmp for faster IO (optional)\n",
        "    src = video_path\n",
        "    local_path = src\n",
        "    if local_copy:\n",
        "        base = Path(src).stem\n",
        "        local_path = f\"/tmp/{base}_local.mp4\"\n",
        "        try:\n",
        "            if os.path.exists(local_path):\n",
        "                os.remove(local_path)\n",
        "            shutil.copy(src, local_path)\n",
        "            print(\"Copied to local:\", local_path)\n",
        "        except Exception as e:\n",
        "            print(\"Warning: local copy failed:\", e)\n",
        "            local_path = src\n",
        "\n",
        "    cap = cv2.VideoCapture(local_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(\"Cannot open video: \" + str(local_path))\n",
        "    orig_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    step = max(1, int(round(orig_fps / target_fps)))\n",
        "    print(f\"orig_fps={orig_fps:.2f} step={step} target_fps={target_fps}\")\n",
        "\n",
        "    # buffers\n",
        "    feat_buffer = []     # stores feature vectors\n",
        "    ts_buffer = []       # stores timestamps\n",
        "    frame_batch = []     # small batch for backbone prediction to avoid per-frame predict calls\n",
        "    batch_batch_size = 8 # tune: number of frames to batch before calling BACKBONE.predict\n",
        "    results = []\n",
        "\n",
        "    idx = 0\n",
        "    frames_processed = 0\n",
        "    t_start = time.time()\n",
        "    last_print = time.time()\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # sample to target fps\n",
        "        if idx % step == 0:\n",
        "            frames_processed += 1\n",
        "            ts = idx / orig_fps\n",
        "            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, (IMG_W, IMG_H))\n",
        "            frame_batch.append(img)\n",
        "            ts_batch_append = ts\n",
        "\n",
        "            # when we have a small batch, run backbone.predict once\n",
        "            if len(frame_batch) >= batch_batch_size:\n",
        "                arr = np.array(frame_batch, dtype=np.float32)\n",
        "                arr = preprocess_input(arr)\n",
        "                t0 = time.time()\n",
        "                feats = BACKBONE.predict(arr, verbose=0)\n",
        "                t1 = time.time()\n",
        "                # append feats in order\n",
        "                for f_idx, fvec in enumerate(feats):\n",
        "                    feat_buffer.append(fvec.astype(np.float32))\n",
        "                    ts_buffer.append(ts - (len(feats)-1 - f_idx) * (1.0/target_fps))  # approximate timestamp\n",
        "                    # maintain buffer length\n",
        "                    if len(feat_buffer) > seq_len:\n",
        "                        feat_buffer.pop(0); ts_buffer.pop(0)\n",
        "                # empty batch\n",
        "                frame_batch = []\n",
        "                # print timing occasionally\n",
        "                if time.time() - last_print > 5.0:\n",
        "                    print(f\"[{datetime.now()}] processed frames={frames_processed}, feat_buf={len(feat_buffer)}, backbone_batch_time={t1-t0:.3f}s\")\n",
        "                    last_print = time.time()\n",
        "\n",
        "            # If buffer full, run temporal model at stride intervals\n",
        "            if len(feat_buffer) == seq_len and (frames_processed % stride == 0):\n",
        "                X = np.array([feat_buffer], dtype=np.float32)\n",
        "                t0m = time.time()\n",
        "                prob = float(model.predict(X, verbose=0)[0,0])\n",
        "                t1m = time.time()\n",
        "                center_ts = ts_buffer[len(ts_buffer)//2]\n",
        "                results.append((center_ts, prob))\n",
        "                if prob >= threshold:\n",
        "                    ts_real = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    log_line = f\"{ts_real} | {video_path} | {center_ts:.2f} | {prob:.4f}\\n\"\n",
        "                    with open(os.path.join(log_dir, \"detections.txt\"), \"a\") as wf:\n",
        "                        wf.write(log_line)\n",
        "                if time.time() - last_print > 5.0:\n",
        "                    print(f\"[TMP] model_pred_time={t1m-t0m:.3f}s prob={prob:.3f} at vt={center_ts:.2f}\")\n",
        "\n",
        "        idx += 1\n",
        "\n",
        "    # flush remaining small frame_batch if any\n",
        "    if len(frame_batch) > 0:\n",
        "        arr = np.array(frame_batch, dtype=np.float32)\n",
        "        arr = preprocess_input(arr)\n",
        "        feats = BACKBONE.predict(arr, verbose=0)\n",
        "        for fvec in feats:\n",
        "            feat_buffer.append(fvec.astype(np.float32))\n",
        "            if len(feat_buffer) > seq_len:\n",
        "                feat_buffer.pop(0)\n",
        "    cap.release()\n",
        "    print(\"Done. total frames processed:\", frames_processed, \"elapsed:\", time.time()-t_start)\n",
        "    return results\n",
        "\n",
        "# Example usage (use your actual video path)\n",
        "res = inference_on_video_fast(\"/content/drive/MyDrive/AccidentProject/data/raw/HueSPfF0dSQ.mp4\", model, target_fps=16, seq_len=48, stride=4, threshold=0.7)\n",
        "print(res[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4R_HKUPUS7E",
        "outputId": "f462244a-cc41-4b5d-fe15-10c367fdcdcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied to local: /tmp/HueSPfF0dSQ_local.mp4\n",
            "orig_fps=25.00 step=2 target_fps=16\n",
            "[2025-11-21 12:37:21.543713] processed frames=8, feat_buf=8, backbone_batch_time=48.366s\n",
            "[2025-11-21 12:37:26.571542] processed frames=120, feat_buf=48, backbone_batch_time=0.110s\n",
            "[TMP] model_pred_time=0.057s prob=1.000 at vt=20.60\n",
            "[TMP] model_pred_time=0.062s prob=1.000 at vt=20.60\n",
            "[2025-11-21 12:37:31.798030] processed frames=288, feat_buf=48, backbone_batch_time=0.082s\n",
            "[TMP] model_pred_time=0.121s prob=0.000 at vt=33.40\n",
            "[TMP] model_pred_time=0.082s prob=0.000 at vt=33.40\n",
            "[2025-11-21 12:37:37.113900] processed frames=448, feat_buf=48, backbone_batch_time=0.117s\n",
            "[2025-11-21 12:37:42.187550] processed frames=576, feat_buf=48, backbone_batch_time=0.082s\n",
            "[TMP] model_pred_time=0.058s prob=1.000 at vt=56.44\n",
            "[2025-11-21 12:37:47.377070] processed frames=736, feat_buf=48, backbone_batch_time=0.083s\n",
            "[TMP] model_pred_time=0.058s prob=0.000 at vt=66.68\n",
            "[2025-11-21 12:37:52.538803] processed frames=864, feat_buf=48, backbone_batch_time=0.081s\n",
            "[2025-11-21 12:37:57.547327] processed frames=1024, feat_buf=48, backbone_batch_time=0.083s\n",
            "[2025-11-21 12:38:02.558479] processed frames=1176, feat_buf=48, backbone_batch_time=0.219s\n",
            "[TMP] model_pred_time=0.060s prob=0.000 at vt=101.88\n",
            "[2025-11-21 12:38:07.686590] processed frames=1304, feat_buf=48, backbone_batch_time=0.090s\n",
            "[TMP] model_pred_time=0.060s prob=1.000 at vt=114.68\n",
            "[2025-11-21 12:38:12.880725] processed frames=1464, feat_buf=48, backbone_batch_time=0.077s\n",
            "[TMP] model_pred_time=0.131s prob=0.000 at vt=124.92\n",
            "[2025-11-21 12:38:18.095198] processed frames=1592, feat_buf=48, backbone_batch_time=0.119s\n",
            "[TMP] model_pred_time=0.060s prob=0.000 at vt=137.72\n",
            "[TMP] model_pred_time=0.068s prob=0.000 at vt=137.72\n",
            "[2025-11-21 12:38:23.317736] processed frames=1752, feat_buf=48, backbone_batch_time=0.078s\n",
            "[TMP] model_pred_time=0.114s prob=0.000 at vt=150.52\n",
            "[2025-11-21 12:38:28.559769] processed frames=1912, feat_buf=48, backbone_batch_time=0.104s\n",
            "[TMP] model_pred_time=0.072s prob=0.000 at vt=160.76\n",
            "[2025-11-21 12:38:33.711508] processed frames=2040, feat_buf=48, backbone_batch_time=0.078s\n",
            "[TMP] model_pred_time=0.106s prob=0.996 at vt=173.56\n",
            "[2025-11-21 12:38:38.879999] processed frames=2200, feat_buf=48, backbone_batch_time=0.076s\n",
            "[TMP] model_pred_time=0.095s prob=0.000 at vt=183.80\n",
            "[2025-11-21 12:38:44.115453] processed frames=2328, feat_buf=48, backbone_batch_time=0.083s\n",
            "[2025-11-21 12:38:49.221884] processed frames=2488, feat_buf=48, backbone_batch_time=0.114s\n",
            "[2025-11-21 12:38:54.282921] processed frames=2640, feat_buf=48, backbone_batch_time=0.131s\n",
            "[TMP] model_pred_time=0.062s prob=0.062 at vt=219.00\n",
            "[2025-11-21 12:38:59.398928] processed frames=2768, feat_buf=48, backbone_batch_time=0.079s\n",
            "[TMP] model_pred_time=0.061s prob=0.000 at vt=231.80\n",
            "[2025-11-21 12:39:04.540084] processed frames=2928, feat_buf=48, backbone_batch_time=0.085s\n",
            "[2025-11-21 12:39:09.587912] processed frames=3040, feat_buf=48, backbone_batch_time=0.120s\n",
            "[TMP] model_pred_time=0.067s prob=1.000 at vt=252.92\n",
            "[2025-11-21 12:39:14.783228] processed frames=3192, feat_buf=48, backbone_batch_time=0.079s\n",
            "[TMP] model_pred_time=0.065s prob=1.000 at vt=265.08\n",
            "[2025-11-21 12:39:19.936686] processed frames=3344, feat_buf=48, backbone_batch_time=0.113s\n",
            "[2025-11-21 12:39:24.990593] processed frames=3464, feat_buf=48, backbone_batch_time=0.084s\n",
            "[2025-11-21 12:39:30.004562] processed frames=3616, feat_buf=48, backbone_batch_time=0.077s\n",
            "[2025-11-21 12:39:35.088252] processed frames=3744, feat_buf=48, backbone_batch_time=0.106s\n",
            "[2025-11-21 12:39:40.112259] processed frames=3888, feat_buf=48, backbone_batch_time=0.080s\n",
            "[2025-11-21 12:39:45.130116] processed frames=4040, feat_buf=48, backbone_batch_time=0.082s\n",
            "[TMP] model_pred_time=0.066s prob=0.999 at vt=331.00\n",
            "[TMP] model_pred_time=0.067s prob=0.999 at vt=331.00\n",
            "[2025-11-21 12:39:50.355349] processed frames=4168, feat_buf=48, backbone_batch_time=0.084s\n",
            "[TMP] model_pred_time=0.067s prob=0.000 at vt=343.80\n",
            "[TMP] model_pred_time=0.064s prob=0.000 at vt=343.80\n",
            "[2025-11-21 12:39:55.608922] processed frames=4328, feat_buf=48, backbone_batch_time=0.082s\n",
            "[TMP] model_pred_time=0.087s prob=0.000 at vt=354.68\n",
            "[2025-11-21 12:40:00.770279] processed frames=4464, feat_buf=48, backbone_batch_time=0.103s\n",
            "[2025-11-21 12:40:05.813130] processed frames=4608, feat_buf=48, backbone_batch_time=0.076s\n",
            "[2025-11-21 12:40:10.882159] processed frames=4760, feat_buf=48, backbone_batch_time=0.080s\n",
            "[2025-11-21 12:40:15.946930] processed frames=4880, feat_buf=48, backbone_batch_time=0.083s\n",
            "[2025-11-21 12:40:21.053474] processed frames=5032, feat_buf=48, backbone_batch_time=0.082s\n",
            "[2025-11-21 12:40:26.059425] processed frames=5168, feat_buf=48, backbone_batch_time=0.115s\n",
            "[TMP] model_pred_time=0.061s prob=1.000 at vt=421.88\n",
            "[TMP] model_pred_time=0.071s prob=1.000 at vt=421.88\n",
            "[2025-11-21 12:40:31.317445] processed frames=5304, feat_buf=48, backbone_batch_time=0.081s\n",
            "[2025-11-21 12:40:36.342324] processed frames=5456, feat_buf=48, backbone_batch_time=0.111s\n",
            "[2025-11-21 12:40:41.376149] processed frames=5576, feat_buf=48, backbone_batch_time=0.082s\n",
            "[2025-11-21 12:40:46.445201] processed frames=5728, feat_buf=48, backbone_batch_time=0.076s\n",
            "[TMP] model_pred_time=0.122s prob=0.997 at vt=467.96\n",
            "[TMP] model_pred_time=0.082s prob=0.997 at vt=467.96\n",
            "[2025-11-21 12:40:51.808576] processed frames=5880, feat_buf=48, backbone_batch_time=0.141s\n",
            "Done. total frames processed: 5994 elapsed: 287.42954087257385\n",
            "[(2.0425, 0.999893307685852), (2.0425, 0.999893307685852), (2.6825, 0.9999328851699829), (2.6825, 0.9999328851699829), (3.3225, 0.9998908042907715), (3.3225, 0.9998908042907715), (3.9625000000000004, 0.9999133348464966), (3.9625000000000004, 0.9999133348464966), (4.6025, 0.9998874664306641), (4.6025, 0.9998874664306641), (5.2425, 0.9998906850814819), (5.2425, 0.9998906850814819), (5.8825, 0.9999034404754639), (5.8825, 0.9999034404754639), (6.5225, 0.999828577041626), (6.5225, 0.999828577041626), (7.1625, 0.016058331355452538), (7.1625, 0.016058331355452538), (7.8025, 6.833529187133536e-05), (7.8025, 6.833529187133536e-05)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15 — save/export final transformer head\n",
        "model.save(os.path.join(EXPORT_DIR, \"transformer_temporal_head.keras\"), include_optimizer=False)\n",
        "print(\"Saved transformer to\", os.path.join(EXPORT_DIR, \"transformer_temporal_head.keras\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbY_A62KUUa1",
        "outputId": "42f40c04-7cc9-4f11-91ac-28c5a5364379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved transformer to /content/drive/MyDrive/AccidentProject/exports/models/transformer_temporal_head.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation cell: window-level + event-level metrics, save CSV + summary\n",
        "import numpy as np, os, pandas as pd, math, itertools\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
        "                             confusion_matrix, classification_report,\n",
        "                             precision_recall_fscore_support, roc_curve,\n",
        "                             precision_recall_curve)\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from datetime import timedelta\n",
        "\n",
        "# --- Params (tweak as needed) ---\n",
        "THRESHOLD = 0.5            # window-level threshold for binary decisions\n",
        "EVENT_TOLERANCE = 3.0      # seconds: predicted event matches GT if overlapping within +/- this many seconds\n",
        "MIN_EVENT_DURATION = 0.5   # seconds: ignore super-short predicted events\n",
        "SAVE_DIR = os.path.join(PROJECT_DIR, \"eval\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# print the uploaded notebook path (developer-provided file)\n",
        "NOTEBOOK_PATH = \"/mnt/data/accident_tsm_full_notebook_reupload.ipynb\"\n",
        "print(\"Notebook file (local):\", NOTEBOOK_PATH)\n",
        "\n",
        "# --- Load validation entries ---\n",
        "if 'tmp_val_idx_file' in globals() and os.path.exists(tmp_val_idx_file):\n",
        "    val_entries = list(np.load(tmp_val_idx_file, allow_pickle=True))\n",
        "else:\n",
        "    entries = list(np.load(index_file, allow_pickle=True))\n",
        "    # fallback: take last 10% as val\n",
        "    n_val = max(1, int(len(entries) * 0.1))\n",
        "    val_entries = entries[:n_val]\n",
        "\n",
        "print(\"Validation windows:\", len(val_entries))\n",
        "\n",
        "# --- Helper to load batch windows and predict in reasonable sized batches ---\n",
        "def predict_on_entries(model, entries, batch_size=64):\n",
        "    y_true = []\n",
        "    y_prob = []\n",
        "    rows = []\n",
        "    feat_dim_local = np.load(entries[0][0], mmap_mode='r').shape[1]\n",
        "    for i in range(0, len(entries), batch_size):\n",
        "        batch = entries[i:i+batch_size]\n",
        "        X = np.zeros((len(batch), SEQ_LEN, feat_dim_local), dtype=np.float32)\n",
        "        for j, (feat_path, start_idx, lbl) in enumerate(batch):\n",
        "            feats = np.load(feat_path, mmap_mode='r')\n",
        "            clip = feats[int(start_idx): int(start_idx) + SEQ_LEN]\n",
        "            if clip.shape[0] != SEQ_LEN:\n",
        "                # pad (shouldn't happen usually)\n",
        "                pad = np.zeros((SEQ_LEN - clip.shape[0], feats.shape[1]), dtype=np.float32)\n",
        "                clip = np.concatenate([clip, pad], axis=0)\n",
        "            X[j] = clip\n",
        "            y_true.append(int(lbl))\n",
        "        preds = model.predict(X, verbose=0).ravel()\n",
        "        y_prob.extend(preds.tolist())\n",
        "        # store rows for CSV: per-window (video basename, start_frame, center_time, true, prob)\n",
        "        for k, (feat_path, start_idx, lbl) in enumerate(batch):\n",
        "            base = os.path.basename(feat_path).replace(\"-feat.npy\", \"\")\n",
        "            center_frame = int(start_idx) + SEQ_LEN // 2\n",
        "            center_time = center_frame / 16.0  # assuming feature fps was 16\n",
        "            rows.append({\n",
        "                \"video\": base,\n",
        "                \"feat_path\": feat_path,\n",
        "                \"start_idx\": int(start_idx),\n",
        "                \"center_time\": center_time,\n",
        "                \"true\": int(lbl),\n",
        "                \"prob\": float(preds[k])\n",
        "            })\n",
        "    return np.array(y_true), np.array(y_prob), pd.DataFrame(rows)\n",
        "\n",
        "# Run predictions on validation\n",
        "y_true, y_prob, df_preds = predict_on_entries(model, val_entries, batch_size=64)\n",
        "print(\"Predictions done. Examples:\\n\", df_preds.head())\n",
        "\n",
        "# --- Window-level metrics ---\n",
        "auc = roc_auc_score(y_true, y_prob) if y_true.sum() > 0 else float('nan')\n",
        "ap = average_precision_score(y_true, y_prob) if y_true.sum() > 0 else float('nan')\n",
        "y_pred = (y_prob >= THRESHOLD).astype(int)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "report = classification_report(y_true, y_pred, zero_division=0)\n",
        "\n",
        "print(\"\\n=== WINDOW-LEVEL METRICS ===\")\n",
        "print(\"Window AUC:\", auc)\n",
        "print(\"Window AP (PR AUC):\", ap)\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(\"Precision:\", precision, \"Recall:\", recall, \"F1:\", f1)\n",
        "print(\"\\nClassification report:\\n\", report)\n",
        "\n",
        "# Save per-window CSV and metrics\n",
        "csv_path = os.path.join(SAVE_DIR, \"window_level_predictions.csv\")\n",
        "df_preds.to_csv(csv_path, index=False)\n",
        "metrics = {\n",
        "    \"window_auc\": float(auc),\n",
        "    \"window_ap\": float(ap),\n",
        "    \"precision\": float(precision),\n",
        "    \"recall\": float(recall),\n",
        "    \"f1\": float(f1),\n",
        "    \"threshold\": float(THRESHOLD),\n",
        "    \"n_val_windows\": int(len(val_entries))\n",
        "}\n",
        "with open(os.path.join(SAVE_DIR, \"window_metrics.json\"), \"w\") as wf:\n",
        "    json.dump(metrics, wf, indent=2)\n",
        "print(\"Saved CSV ->\", csv_path)\n",
        "print(\"Saved metrics ->\", os.path.join(SAVE_DIR, \"window_metrics.json\"))\n",
        "\n",
        "# --- Plot ROC and PR curves (quick) ---\n",
        "fpr, tpr, _ = roc_curve(y_true, y_prob) if len(np.unique(y_true))>1 else (None, None, None)\n",
        "precisions, recalls, _ = precision_recall_curve(y_true, y_prob)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "if fpr is not None:\n",
        "    plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC\")\n",
        "    plt.legend()\n",
        "else:\n",
        "    plt.text(0.2,0.5,\"Not enough class variety for ROC\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(recalls, precisions, label=f\"AP={ap:.3f}\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision-Recall\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# === EVENT-LEVEL METRICS ===\n",
        "# Convert ground-truth windows to per-video event lists (from val_entries -> use underlying txt intervals)\n",
        "def build_video_intervals_from_index(entries):\n",
        "    # returns dict: video_basename -> list of (start_sec, end_sec) ground truth intervals\n",
        "    vid_intervals = {}\n",
        "    for feat_path, start_idx, lbl in entries:\n",
        "        base = os.path.basename(feat_path).replace(\"-feat.npy\", \"\")\n",
        "        # find raw txt path in DATA_RAW with same base (common extensions)\n",
        "        txt_path = None\n",
        "        for ext in ('.txt', '.label'):\n",
        "            p = os.path.join(DATA_RAW, base + ext)\n",
        "            if os.path.exists(p):\n",
        "                txt_path = p; break\n",
        "        if base not in vid_intervals:\n",
        "            vid_intervals[base] = set()\n",
        "        if int(lbl) == 1:\n",
        "            # the window corresponds to a positive; convert window start/end times\n",
        "            s_frame = int(start_idx)\n",
        "            e_frame = int(start_idx) + SEQ_LEN - 1\n",
        "            s_sec = s_frame / 16.0\n",
        "            e_sec = e_frame / 16.0\n",
        "            # store interval with small merge tolerance\n",
        "            vid_intervals[base].add((round(s_sec,3), round(e_sec,3)))\n",
        "    # Convert sets -> merged sorted lists\n",
        "    for k in list(vid_intervals.keys()):\n",
        "        ints = sorted(list(vid_intervals[k]))\n",
        "        merged = []\n",
        "        for a,b in ints:\n",
        "            if not merged or a > merged[-1][1] + 1e-6:\n",
        "                merged.append([a,b])\n",
        "            else:\n",
        "                merged[-1][1] = max(merged[-1][1], b)\n",
        "        vid_intervals[k] = [(x[0], x[1]) for x in merged]\n",
        "    return vid_intervals\n",
        "\n",
        "# Build predicted events per-video by merging contiguous predicted positive windows\n",
        "def build_predicted_events(df_preds, threshold=THRESHOLD, tolerance_merge= (SEQ_LEN/16.0)/2.0):\n",
        "    # tolerance_merge approx half-window so adjacent windows join\n",
        "    pred_events = {}\n",
        "    df_pos = df_preds[df_preds['prob'] >= threshold].copy()\n",
        "    for base, grp in df_pos.groupby('video'):\n",
        "        times = sorted(grp['center_time'].tolist())\n",
        "        # merge times into continuous events if centers are within tolerance_merge seconds\n",
        "        events = []\n",
        "        for t in times:\n",
        "            if not events:\n",
        "                events.append([t - SEQ_LEN/16.0/2.0, t + SEQ_LEN/16.0/2.0])\n",
        "            else:\n",
        "                if t <= events[-1][1] + tolerance_merge:\n",
        "                    # extend\n",
        "                    events[-1][1] = max(events[-1][1], t + SEQ_LEN/16.0/2.0)\n",
        "                else:\n",
        "                    events.append([t - SEQ_LEN/16.0/2.0, t + SEQ_LEN/16.0/2.0])\n",
        "        # filter tiny events\n",
        "        filtered = [(max(0, round(a,3)), round(b,3)) for a,b in events if (b - a) >= MIN_EVENT_DURATION]\n",
        "        pred_events[base] = filtered\n",
        "    return pred_events\n",
        "\n",
        "# Ground-truth: parse .txt intervals for each video (full GT intervals)\n",
        "def load_gt_intervals_for_val(entries):\n",
        "    gt = {}\n",
        "    bases = set([os.path.basename(e[0]).replace(\"-feat.npy\",\"\") for e in entries])\n",
        "    for base in bases:\n",
        "        # find txt file\n",
        "        txt_path = None\n",
        "        for ext in ('.txt', '.label'):\n",
        "            p = os.path.join(DATA_RAW, base + ext)\n",
        "            if os.path.exists(p): txt_path = p; break\n",
        "        intervals = []\n",
        "        if txt_path:\n",
        "            for line in open(txt_path,'r'):\n",
        "                line=line.strip()\n",
        "                if not line: continue\n",
        "                if \"-\" in line:\n",
        "                    a,b = line.split(\"-\")\n",
        "                elif \"to\" in line:\n",
        "                    a,b = line.split(\"to\")\n",
        "                else:\n",
        "                    continue\n",
        "                try:\n",
        "                    intervals.append((to_seconds(a), to_seconds(b)))\n",
        "                except:\n",
        "                    continue\n",
        "        gt[base] = intervals\n",
        "    return gt\n",
        "\n",
        "gt_intervals = load_gt_intervals_for_val(val_entries)\n",
        "pred_events = build_predicted_events(df_preds, threshold=THRESHOLD)\n",
        "\n",
        "# Event-level matching. A predicted event is a true positive if it overlaps any GT interval by > 0 seconds within tolerance\n",
        "def overlap(a_start,a_end,b_start,b_end):\n",
        "    return not (a_end < b_start or b_end < a_start)\n",
        "\n",
        "event_TP = 0; event_FP = 0; event_FN = 0\n",
        "per_video_matches = {}\n",
        "for video, gt_list in gt_intervals.items():\n",
        "    preds = pred_events.get(video, [])\n",
        "    matched_gt = set()\n",
        "    for pe in preds:\n",
        "        p_s,p_e = pe\n",
        "        matched = False\n",
        "        for gi_idx, (g_s,g_e) in enumerate(gt_list):\n",
        "            # consider match if overlap within tolerance or center within +/- EVENT_TOLERANCE\n",
        "            if overlap(p_s, p_e, g_s - EVENT_TOLERANCE, g_e + EVENT_TOLERANCE):\n",
        "                matched = True\n",
        "                matched_gt.add(gi_idx)\n",
        "                break\n",
        "        if matched:\n",
        "            event_TP += 1\n",
        "        else:\n",
        "            event_FP += 1\n",
        "    # ground-truth events that were not matched\n",
        "    event_FN += max(0, len(gt_list) - len(matched_gt))\n",
        "    per_video_matches[video] = {\"gt\": gt_list, \"pred\": preds, \"matched_gt_count\": len(matched_gt)}\n",
        "\n",
        "# compute event-level metrics\n",
        "ev_precision = event_TP / (event_TP + event_FP) if (event_TP + event_FP)>0 else 0.0\n",
        "ev_recall = event_TP / (event_TP + event_FN) if (event_TP + event_FN)>0 else 0.0\n",
        "ev_f1 = 2*ev_precision*ev_recall/(ev_precision+ev_recall) if (ev_precision+ev_recall)>0 else 0.0\n",
        "\n",
        "print(\"\\n=== EVENT-LEVEL METRICS ===\")\n",
        "print(\"Event TP:\", event_TP, \"FP:\", event_FP, \"FN:\", event_FN)\n",
        "print(\"Event Precision:\", round(ev_precision,4), \"Event Recall:\", round(ev_recall,4), \"Event F1:\", round(ev_f1,4))\n",
        "\n",
        "# Save event-level results and pred_events/gt to JSON\n",
        "with open(os.path.join(SAVE_DIR, \"event_results.json\"), \"w\") as wf:\n",
        "    json.dump({\n",
        "        \"event_tp\": int(event_TP),\n",
        "        \"event_fp\": int(event_FP),\n",
        "        \"event_fn\": int(event_FN),\n",
        "        \"event_precision\": ev_precision,\n",
        "        \"event_recall\": ev_recall,\n",
        "        \"event_f1\": ev_f1,\n",
        "        \"pred_events\": pred_events,\n",
        "        \"gt_intervals\": gt_intervals,\n",
        "        \"per_video_matches\": per_video_matches\n",
        "    }, wf, indent=2)\n",
        "\n",
        "print(\"Saved event-level JSON ->\", os.path.join(SAVE_DIR, \"event_results.json\"))\n",
        "\n",
        "# Save a human-readable summary\n",
        "with open(os.path.join(SAVE_DIR, \"eval_summary.txt\"), \"w\") as wf:\n",
        "    wf.write(\"=== WINDOW-LEVEL METRICS ===\\n\")\n",
        "    wf.write(json.dumps(metrics, indent=2) + \"\\n\\n\")\n",
        "    wf.write(\"Confusion matrix:\\n\" + str(cm) + \"\\n\\n\")\n",
        "    wf.write(\"Classification report:\\n\" + report + \"\\n\\n\")\n",
        "    wf.write(\"=== EVENT-LEVEL METRICS ===\\n\")\n",
        "    wf.write(f\"event_tp: {event_TP}\\nevent_fp: {event_FP}\\nevent_fn: {event_FN}\\nevent_precision: {ev_precision}\\nevent_recall: {ev_recall}\\nevent_f1: {ev_f1}\\n\")\n",
        "print(\"Saved human summary ->\", os.path.join(SAVE_DIR, \"eval_summary.txt\"))\n",
        "\n",
        "print(\"\\nDone. CSV and JSON results are in:\", SAVE_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e2ti8wBOuwPX",
        "outputId": "f905129c-a067-4518-ed6b-6b1306957727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook file (local): /mnt/data/accident_tsm_full_notebook_reupload.ipynb\n",
            "Validation windows: 1287\n",
            "Predictions done. Examples:\n",
            "                  video                                          feat_path  \\\n",
            "0          HueSPfF0dSQ  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "1          3LsNpm5y-VA  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "2  Copy of bCr5Grrfw4I  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "3  Copy of hZm7dC8qBO0  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "4          AuQz_-J2kzc  /content/drive/MyDrive/AccidentProject/data/fe...   \n",
            "\n",
            "   start_idx  center_time  true          prob  \n",
            "0       1040         66.5     0  3.033526e-06  \n",
            "1       6072        381.0     0  2.280039e-06  \n",
            "2       7520        471.5     0  2.783033e-07  \n",
            "3       7280        456.5     0  3.886765e-07  \n",
            "4       8328        522.0     1  8.338495e-01  \n",
            "\n",
            "=== WINDOW-LEVEL METRICS ===\n",
            "Window AUC: 0.9836104029709853\n",
            "Window AP (PR AUC): 0.9634113577747099\n",
            "Confusion matrix:\n",
            " [[824  65]\n",
            " [ 38 360]]\n",
            "Precision: 0.8470588235294118 Recall: 0.9045226130653267 F1: 0.8748481166464156\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.94       889\n",
            "           1       0.85      0.90      0.87       398\n",
            "\n",
            "    accuracy                           0.92      1287\n",
            "   macro avg       0.90      0.92      0.91      1287\n",
            "weighted avg       0.92      0.92      0.92      1287\n",
            "\n",
            "Saved CSV -> /content/drive/MyDrive/AccidentProject/eval/window_level_predictions.csv\n",
            "Saved metrics -> /content/drive/MyDrive/AccidentProject/eval/window_metrics.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAHWCAYAAAALjsguAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbHBJREFUeJzt3X98jfX/x/Hn2dnO2YZtmG1ozG+RHyFrJKkxkdJPnxRSKbJ+WCm/hU9GRSS1Euk3KfXpE/HRSiUr3/zoF4n8TDaEjY39OOf6/iEnp21yZjvX2fa4327nZue63td1XteVvPc81/t6XxbDMAwBAAAAAADT+ZldAAAAAAAAOIWQDgAAAACAjyCkAwAAAADgIwjpAAAAAAD4CEI6AAAAAAA+gpAOAAAAAICPIKQDAAAAAOAjCOkAAAAAAPgIQjoAAAAAAD6CkA4AAACUI3fccYdiYmI82mb16tWyWCxavXp1mdRU3l1xxRW64oorXO937doli8WihQsXmlYTKi9COlCJLVy4UBaLxfXy9/dX3bp1dccdd2jfvn2F2huGoddff12XX365wsLCFBwcrFatWmny5MnKzs4u9nPef/99XX311QoPD5fNZlOdOnV0yy236NNPPy3LwwMAoNT8vc8MDAxU06ZNlZiYqIyMDLPL82mnA+/pl5+fn2rUqKGrr75aaWlpZpcH+Bx/swsAYL7JkyerQYMGOnnypL7++mstXLhQa9as0Y8//qjAwEBJksPhUP/+/fXOO++oS5cuevzxxxUcHKwvv/xSkyZN0pIlS/TJJ58oMjLStV/DMHTnnXdq4cKFuvjii5WUlKSoqCjt379f77//vq666ip99dVX6tSpk1mHDgCAR87sM9esWaMXXnhBy5cv148//qjg4GCv1DBv3jw5nU6Ptrn88st14sQJ2Wy2Mqrqn916663q1auXHA6HfvnlFz3//PPq1q2b/u///k+tWrUyrS7A1xDSAejqq69Whw4dJEl33323wsPDNX36dH344Ye65ZZbJElPPvmk3nnnHT3yyCN66qmnXNvec889uuWWW9S3b1/dcccd+vjjj13rZsyYoYULF+qhhx7SzJkzZbFYXOvGjh2r119/Xf7+/DMEACg//t5n1qxZUzNnztR//vMf3XrrrYXaZ2dnq0qVKqVaQ0BAgMfb+Pn5ub54N0u7du10++23u9536dJFV199tV544QU9//zzJlYG+BaGuwMopEuXLpKkX3/9VZJ04sQJPfXUU2ratKmSk5MLte/Tp48GDRqkFStW6Ouvv3Ztk5ycrObNm+vpp592C+inDRgwQB07dizDIwEAoGxdeeWVkqSdO3fqjjvuUNWqVfXrr7+qV69eqlatmm677TZJktPp1KxZs9SyZUsFBgYqMjJS9957r44cOVJonx9//LG6du2qatWqKSQkRJdcconeeust1/qi7klftGiR2rdv79qmVatWmj17tmt9cfekL1myRO3bt1dQUJDCw8N1++23F7rl7fRx7du3T3379lXVqlVVq1YtPfLII3I4HCU+d3//feO0o0eP6qGHHlJ0dLTsdrsaN26s6dOnFxo94HQ6NXv2bLVq1UqBgYGqVauWevbsqW+//dbV5pVXXtGVV16piIgI2e12tWjRQi+88EKJawa8gZAOoJBdu3ZJkqpXry5JWrNmjY4cOaL+/fsXe+V74MCBkqSPPvrItc3hw4fVv39/Wa3Wsi8aAAATnA6YNWvWlCQVFBQoISFBERERevrpp3XjjTdKku69916NHDlSnTt31uzZszV48GC9+eabSkhIUH5+vmt/CxcuVO/evXX48GGNHj1a06ZNU9u2bbVixYpia1i1apVuvfVWVa9eXdOnT9e0adN0xRVX6Kuvvjpr7QsXLtQtt9wiq9Wq5ORkDRkyREuXLtVll12mo0ePurV1OBxKSEhQzZo19fTTT6tr166aMWOGXnrppZKcNkmFf9+QpJycHHXt2lVvvPGGBg4cqGeffVadO3fW6NGjlZSU5Lb9XXfd5Qrz06dP16hRoxQYGOi6YCBJL7zwgurXr68xY8ZoxowZio6O1n333ae5c+eWuG6grDHOFIAyMzN16NAhnTx5Ut98840mTZoku92ua665RpK0efNmSVKbNm2K3cfpdVu2bHH7k3vMAAAVyZl95ldffaXJkycrKChI11xzjdLS0pSbm6ubb77ZbeTZmjVr9PLLL+vNN99U//79Xcu7deumnj17asmSJerfv78yMzP1wAMPqGPHjlq9erXb8HTDMIqtadmyZQoJCdHKlSvP+Yvx/Px8PfbYY7rooov0xRdfuD7rsssu0zXXXKNnnnlGkyZNcrU/efKk+vXrp/Hjx0uShg4dqnbt2mn+/PkaNmzYOX1mTk6ODh06JIfDoW3btrlC90033eRqM3PmTP3666/auHGjmjRpIunUFxx16tTRU089pYcffljR0dH67LPPtHDhQj3wwANuIwYefvhht3P1+eefKygoyPU+MTFRPXv21MyZMzV8+PBzqhvwNq6kA1B8fLxq1aql6Oho3XTTTapSpYo+/PBDXXDBBZKkY8eOSZKqVatW7D5Or8vKynL782zbAABQ3pzZZ/7rX/9S1apV9f7776tu3bquNn8PrUuWLFFoaKi6d++uQ4cOuV7t27dX1apV9dlnn0k6dUX82LFjrivCZyrqtrHTwsLClJ2drVWrVp3zcXz77bc6cOCA7rvvPrfP6t27t5o3b65ly5YV2mbo0KFu77t06aIdO3ac82dOnDhRtWrVUlRUlLp06aItW7ZoxowZbiF9yZIl6tKli6pXr+52ruLj4+VwOPTFF19Ikt577z1ZLBZNnDix0Oecea7ODOinv2Dp2rWrduzYoczMzHOuHfAmrqQD0Ny5c9W0aVNlZmZqwYIF+uKLL2S3213rTwft02G9KH8P8iEhIf+4DQAA5c3pPtPf31+RkZFq1qyZ/Pz+uu7l7+/v+pL7tG3btikzM1MRERFF7vPAgQOS/ho6f9FFF3lU03333ad33nlHV199terWrasePXrolltuUc+ePYvdZvfu3ZKkZs2aFVrXvHlzrVmzxm3Z6Xu+z1S9enW3e+oPHjzodo961apVVbVqVdf7e+65RzfffLNOnjypTz/9VM8++2yhe9q3bdum77//vtBnnXbmuapTp45q1KhR7DFK0ldffaWJEycqLS1NOTk5busyMzMVGhp61u0BMxDSAahjx46umWr79u2ryy67TP3799fWrVtVtWpVXXjhhZKk77//Xn379i1yH99//70kqUWLFpJOdfCS9MMPPxS7DQAA5c2ZfWZR7Ha7W2iXTk1wFhERoTfffLPIbYoLpOcqIiJCmzZt0sqVK/Xxxx/r448/1iuvvKKBAwfq1VdfPa99n3Yuw+gvueQSV/iXTl05f/zxx13vmzRpovj4eEnSNddcI6vVqlGjRqlbt26uc+p0OtW9e3c9+uijRX5G06ZNz7nmX3/9VVdddZWaN2+umTNnKjo6WjabTcuXL9czzzzj8WPsAG8hpANwc3rymG7duum5557TqFGjdNlllyksLExvvfWWxo4dW2RH/dprr0mS6z72yy67TNWrV9fbb7+tMWPGMHkcAKDSatSokT755BN17tzZbfh1Ue0k6ccff1Tjxo09+gybzaY+ffqoT58+cjqduu+++/Tiiy9q/PjxRe6rfv36kqStW7e6Zqg/bevWra71nnjzzTd14sQJ1/uGDRuetf3YsWM1b948jRs3zjUxXqNGjXT8+HFXmC9Oo0aNtHLlSh0+fLjYq+n//e9/lZubqw8//FD16tVzLT99ewHgq7gnHUAhV1xxhTp27KhZs2bp5MmTCg4O1iOPPKKtW7dq7NixhdovW7ZMCxcuVEJCgi699FJJUnBwsB577DFt2bJFjz32WJET3rzxxhtat25dmR8PAABmuuWWW+RwODRlypRC6woKClwzqffo0UPVqlVTcnKyTp486dbubBPH/fHHH27v/fz81Lp1a0lSbm5ukdt06NBBERERSklJcWvz8ccfa8uWLerdu/c5HduZOnfurPj4eNfrn0J6WFiY7r33Xq1cuVKbNm2SdOpcpaWlaeXKlYXaHz16VAUFBZKkG2+8UYZhuE1ud9rpc3X6AsGZ5y4zM1OvvPKKx8cGeBNX0gEUaeTIkbr55pu1cOFCDR06VKNGjdLGjRs1ffp0paWl6cYbb1RQUJDWrFmjN954QxdeeGGhIXUjR47UTz/9pBkzZuizzz7TTTfdpKioKKWnp+uDDz7QunXrtHbtWpOOEAAA7+jatavuvfdeJScna9OmTerRo4cCAgK0bds2LVmyRLNnz9ZNN92kkJAQPfPMM7r77rt1ySWXqH///qpevbq+++475eTkFDt0/e6779bhw4d15ZVX6oILLtDu3bs1Z84ctW3b1nXL2t8FBARo+vTpGjx4sLp27apbb71VGRkZmj17tmJiYjRixIiyPCUuDz74oGbNmqVp06Zp0aJFGjlypD788ENdc801uuOOO9S+fXtlZ2frhx9+0Lvvvqtdu3YpPDxc3bp104ABA/Tss89q27Zt6tmzp5xOp7788kt169ZNiYmJ6tGjh2uEwb333qvjx49r3rx5ioiI0P79+71yfEBJENIBFOmGG25Qo0aN9PTTT2vIkCGyWq1655139Nprr+nll1/W+PHjlZeXp0aNGmnixIl6+OGHVaVKFbd9+Pn56bXXXtN1112nl156SU8//bSysrJUq1YtXX755XryyScVFxdn0hECAOA9KSkpat++vV588UWNGTNG/v7+iomJ0e23367OnTu72t11112KiIjQtGnTNGXKFAUEBKh58+ZnDc233367XnrpJT3//PM6evSooqKi1K9fPz3++OOF7o8/0x133KHg4GBNmzZNjz32mKpUqaLrr79e06dPV1hYWGkefrHq1Kmj/v376/XXX9evv/6qRo0a6fPPP9fUqVO1ZMkSvfbaawoJCVHTpk01adIkt4neXnnlFbVu3Vrz58/XyJEjFRoaqg4dOqhTp06STk2K9+6772rcuHF65JFHFBUVpWHDhqlWrVq68847vXJ8QElYjLONnQEAAAAAAF7DPekAAAAAAPgIQjoAAAAAAD6CkA4AAAAAgI8gpAMAAAAA4CMI6QAAAAAA+AhCOgAAAAAAPqLSPSfd6XTq999/V7Vq1WSxWMwuBwAAGYahY8eOqU6dOmd9pjHOHf09AMCXeNLXV7qQ/vvvvys6OtrsMgAAKGTv3r264IILzC6jQqC/BwD4onPp6ytdSK9WrZqkUycnJCTE5GoAAJCysrIUHR3t6qNw/ujvAQC+xJO+vtKF9NND3kJCQui0AQA+hWHZpYf+HgDgi86lr+fGNwAAAAAAfAQhHQAAAAAAH0FIBwAAAADAR1S6e9IBAAAAwGyGYaigoEAOh8PsUlBKAgICZLVaz3s/hHQAAAAA8KK8vDzt379fOTk5ZpeCUmSxWHTBBReoatWq57UfQjoAAAAAeInT6dTOnTtltVpVp04d2Ww2nu5RARiGoYMHD+q3335TkyZNzuuKOiEdAAAAALwkLy9PTqdT0dHRCg4ONrsclKJatWpp165dys/PP6+QzsRxAAAAAOBlfn5EsYqmtEZE8DcDAAAAAAAfQUgHAAAAAMBHmBrSv/jiC/Xp00d16tSRxWLRBx988I/brF69Wu3atZPdblfjxo21cOHCMq8TAACUDH09AACeMTWkZ2dnq02bNpo7d+45td+5c6d69+6tbt26adOmTXrooYd09913a+XKlWVcKQAAKAn6egCoeNLS0mS1WtW7d2+35bt27ZLFYnG9atasqR49emjjxo0l/qw9e/aod+/eCg4OVkREhEaOHKmCgoKzbrNhwwZ1795dYWFhqlmzpu655x4dP368ULuFCxeqdevWCgwMVEREhIYPH+5at3XrVnXr1k2RkZEKDAxUw4YNNW7cOOXn55f4WM6VqbO7X3311br66qvPuX1KSooaNGigGTNmSJIuvPBCrVmzRs8884wSEhLKqkyYwDAMnch3mF0GAPyjoAArj845C/p6AKh45s+fr/vvv1/z58/X77//rjp16rit/+STT9SyZUv99ttveuCBB3T11Vfr559/VlhYmEef43A41Lt3b0VFRWnt2rXav3+/Bg4cqICAAE2dOrXIbX7//XfFx8erX79+eu6555SVlaWHHnpId9xxh959911Xu5kzZ2rGjBl66qmnFBsbq+zsbO3atcu1PiAgQAMHDlS7du0UFham7777TkOGDJHT6Sz2s0tLuXoEW1pamuLj492WJSQk6KGHHip2m9zcXOXm5rreZ2VllVV5KIangdswpJtT0rR5P/+tAPi+zZMTFGwrV92pTytJXy+VXX//7a7DGvfBj6WyL5QPNarYNPOWtooKDTS7FFQiZl6g8vTL5uPHj2vx4sX69ttvlZ6eroULF2rMmDFubWrWrKmoqChFRUXp6aefVufOnfXNN994/GXr//73P23evFmffPKJIiMj1bZtW02ZMkWPPfaYHn/8cdlstkLbfPTRRwoICNDcuXNdM+inpKSodevW2r59uxo3bqwjR45o3Lhx+u9//6urrrrKtW3r1q1dPzds2FANGzZ0va9fv75Wr16tL7/80qNjKIly9VtFenq6IiMj3ZZFRkYqKytLJ06cUFBQUKFtkpOTNWnSJG+VWCmd7R8VAjcAwBMl6eulsuvvs/Mc+jn9WKnvF75t9dYD+lfHemaXgUrkRL5DLSaYc1uPp182v/POO2revLmaNWum22+/XQ899JBGjx5dbNA//e92Xl6eJGno0KF64403zvoZp4emp6WlqVWrVm79QkJCgoYNG6affvpJF198caFtc3NzZbPZ3B5xd7qGNWvWqHHjxlq1apWcTqf27dunCy+8UMeOHVOnTp00Y8YMRUdHF1nT9u3btWLFCt1www1nrb00lKuQXhKjR49WUlKS631WVlaxJx5FMyuEt6gdoiVD48QoUgC+LCjAanYJUNn1963rhuqNu2LPez8oH5755Bet331ETsPsSs6PYRhyGpLDaZx6GYYcjlN/FjidcjrltszhdMrhlGtdgdMpp2EUuazAYZz68899/32Z0/nXutOffXqZ02moTliQ+l0SzW1C5dj8+fN1++23S5J69uypzMxMff7557riiisKtT169KimTJmiqlWrqmPHjpKkyZMn65FHHjmnzyrui9vT64py5ZVXKikpSU899ZQefPBBZWdna9SoUZKk/fv3S5J27NjhGrY+e/ZshYaGaty4cerevbu+//57tyv0nTp10oYNG5Sbm6t77rlHkydPPqfaz0e5CulRUVHKyMhwW5aRkaGQkJBiv1m32+2y2+3eKK/c8GQ4TWmF8JIEbu7zBIDKpyR9vVR2/X31KjZd1iS81PcL3/Ra2q5CyxxOQ3kFTuUVOJXrcPz1859/5jmc7sscTuUXOFXgdCrfYajAcerPfKdTBaffO40/2xjKd5xanv/n8tPtT23/1z5OtT3d/q82BQ5DeX/uwxXIffxbhua1Q9Q2OszsMnxKUIBVmyebM++GJ182b926VevWrdP7778vSfL391e/fv00f/58t5DeqVMn+fn5KTs7Ww0bNtTixYtd4ToiIkIRERGlegxnatmypV599VUlJSVp9OjRslqteuCBBxQZGem6uu50OpWfn69nn31WPXr0kCS9/fbbioqK0meffeY2LH/x4sU6duyYvvvuO40cOVJPP/20Hn300TKrXypnIT0uLk7Lly93W7Zq1SrFxcWZVJHv+3sgL6sr3/8UwgncAIBzQV8PXzD5o5805aPNynM4fT7wloS/n0XWv78sRSwrbvm5Lvvb8mU/7NfRnHx9veMPHcnJc/uiomWdUDWOqGr2qTGNxWIpF/ObzJ8/XwUFBW4TxRmGIbvdrueee861bPHixWrRooVq1qxZaLI4T4a7R0VFad26dW7rTn+RGxUVVez2/fv3V//+/ZWRkaEqVarIYrFo5syZrnvMa9euLUlq0aKFa5tatWopPDxce/bscdvX6VFZLVq0kMPh0D333KOHH35YVmvZjaQz9W/C8ePHtX37dtf7nTt3atOmTapRo4bq1aun0aNHa9++fXrttdcknfoP+txzz+nRRx/VnXfeqU8//VTvvPOOli1bZtYh+DSn09A1c9aUSiAnhAMASoK+HuVJ44iq+t/mDJ3Mdxa53mKRbFY/2fz9ZPf3O+Nnq2z+p362Wf0U4O+nAD+L/K0W+VtPLfP3O/VzgNUifz8/BfhbFODnJ3+rRQFnLv9zG38/i2z+fvJ3tTm9vqg2p/ZRKBxb/xaeLRb5+Zn3+9qGPUd1NCdf0z7+udC6anZ/fTs+XnZ/biHyVQUFBXrttdc0Y8YM19Xn0/r27au3335bPXv2lHQq2DZq1KjI/Xgy3D0uLk5PPPGEDhw44Lr6vmrVKoWEhLgF7OKcvnq/YMECBQYGqnv37pKkzp07Szo1MuCCCy6QJB0+fFiHDh1S/fr1i93f6SvwTqez4ob0b7/9Vt26dXO9P30v2aBBg7Rw4ULt37/f7ZuMBg0aaNmyZRoxYoRmz56tCy64QC+//DKPZCmC02noqpmfa+eh7CLXezr8nBAOACgJ+nqUJyMTmumGdhdIMmSz/hW87X/+6e9n4feh8zAwrr4WrNkp6+kvMPxOndNvdx/RsdwCbdh9VAFWi+vWgdyCUyMZLm1YQzWrcvuq2T766CMdOXJEd911l0JDQ93W3XjjjZo/f74rpJ+NJ8Pde/TooRYtWmjAgAF68sknlZ6ernHjxmn48OGuW5zWrVungQMHKjU1VXXr1pUkPffcc+rUqZOqVq2qVatWaeTIkZo2bZrrqn7Tpk113XXX6cEHH9RLL72kkJAQjR49Ws2bN3f1WW+++aYCAgLUqlUr2e12ffvttxo9erT69eungICAcz1tJWIxDKPijeE5i6ysLIWGhiozM1MhISFml1MmDMNQ72f/uoLeILyKPrr/MrdATugGAN9RGfomb+OcAuVDXoFTTcd9fNY2XZqE6/UKNIHjyZMntXPnTjVo0ECBgeXnUX99+vSR0+kscmTTunXrFBsbq++++05t2rTRxo0b1bZt21L53N27d2vYsGFavXq1qlSpokGDBmnatGny9z91vXn16tXq1q2bdu7cqZiYGEnSwIEDtWzZMh0/flzNmzfXI488ogEDBrjtNysrSyNGjNDSpUvl5+enrl27avbs2a7h7YsXL9aTTz6pX375RYZhqH79+rr99ts1YsSIYv+7ne2/rSf9EiG9AsrJK3A9wqFBeBWlJnU1dWgTAODsKkPf5G2cU6D8SFq8Sat/OSj7GaMW7P5Wncx3aNuB47qgepAmXdtSJ/IdrlsRureIVGhQ2V7NLCvlNaTjn5VWSPf92QngsTO/dvno/ssI6AAAAPBZM/u1LXL52u2H1P/lb/TbkRO669Vv3dYN7hyjiX1aeqE6wPsI6RWIYRjKyXPomjlrXMsY0Q4AAIDyqG29MHVpEq4DWbkKDPBTYIBVB4/nasfBbK3ffUQL1uzUiXyHcvIKlJPn0Ik8x5/vT/3sWv7nsiYRVfXanR3lb/Uz+9CAsyKkVxCGYeimlDSt333EtaxF7RCPnnsIAAAA+Ipgm3+he9FfS9ulCf/5Sd//lqnvf8v0aH8Hj+Xqx9+zVDs0UNm5pwJ8zhlh3u3nP9dn5zkkGbqzcwM1iaxWikcHFI+QXkGcyHcUCuinJovjUjoAAAAqhp4to/TtriPKyStQkM1fwQFWBdlOvU7/HGzzV7DNqsAAq4Jtp16DF/6fjp0sUN+5X5Xocwschp66uU0pHw1QNEJ6BXHmfejfjotXzSo2AjoAAAAqlIiQQD1768Ueb3dpw5patTlDkuRnkarY/BVks6qK3d8V5E+He9efdqu27D+mL345qP9+/7u+3vmHpt3QWp0bh5fKsVSy+bsrhdL6b0pIrwCcTsPtPvRgG49XAwAAAE57aUB7Hc3JV5DNKru/3zn/rvzJ5gx98ctBncx3au/hE5q/Zud5h/TTz9jOyclRUFDQee0LviUvL0+SZLWe3y3HhPRyzuk0dNXMz7XzULYk7kMHAAAA/s5isah6FZvH2111YYRWPnS5Xv5yh5as/02f/nxA73y7V7d0iC5xLVarVWFhYTpw4IAkKTg4mAtsFYDT6dTBgwcVHBzseoZ7SRHSyzHDOHUF/XRAbxBehfvQAQAAgFJisVjULKqabo2tpyXrf5MkLVq3R78eOK6sk/nKOlmgYycLdOxkvqoH2/TsrRerqv2fI1ZUVJQkuYI6KgY/Pz/Vq1fvvPMYIb0cO5Hv0Ob9WZJOBfTUpK48Ex0AAAAoZe3qVdf9VzbWnE+3a8Oeo9qw52iR7ZZ8u1fNo0JOBfgTp0L8hVHV1OlvQ+QtFotq166tiIgI5efne+EI4A02m01+fuf/iD9Cejl25rwEH91/GQEdAAAAKCP9LonWoeO5cjqlkCB/VQsMULXAU3/O/Wy7dh7K1qT/bi60XYDVog3ju6taYEChdVar9bzvX0bFQ0gvpwzD0M0paa73jHAHAAAAys4F1YOVfEPrItf9cTxXcz7driCbVSGB/goJClBIYIA+/+Wg8h2GRizepJcHXeLlilFeEdLLqTOHujNZHAAAAGCee7s20r1dGxVa3nnap9p39IQ+2XJAOw9lq0F4FROqQ3lz/gPmYbolQ+OYLA4AAADwMe/f18n18zXPfimH86/7VQ3DUOaJfO07eoJnpsMNV9IrAPI5AAAA4HsiQgJ1Q7u6Wrphn7LzHLrlxTRlnsjXkew8HT2R7wrtt3asp+QbWplcLXwFIR0AAAAAykjyDa204sd05eQ5tH73kSLbLP9hvzKyTupEnkOJVzZW57/NBo/KhZBeTjEiBgAAAPB9dn+r3rw7Vpv3Z6l6sE1hwQGqUcWm6sE2bcs4rtvnf6PME/n69OdTz0zfmnFMaaOvlN2fOacqK0J6OfT3md0BAAAA+K6L61XXxfWqF1peq6pdU69vpcwT+fol45je37hPh7PzNHPVLxp99YUmVApfQEgvZwzD0B/ZeczsDgAAAJRzfn4W9Y+tJ0n6/egJvb9xnyTp9bTdWvljup64vhVD3yshZncvRwzD0E0paerw709cy5jZHQAAACj/6oQF6YXb2kmScvIc2vVHjlb8mG5yVTADIb0cOZHvPtlEh/rVFWzjKjoAAABQESS0jNLiey5V37Z1JEmGDGWdzNeOg8e161C2ydXBWxjuXk59Oy5eNavYuIoOAAAAVBB+fhbFNqyptb/+IUl64+s9euPrPa71Lw/soPgWkWaVBy/hSno5FWyzEtABAACACqhhrSpFLr/7tW/Vc9YX2nf0hJcrgjdxJR0AAAAAfMh1beuqzQVh8rNYFF7Nptmp2/Ti5zskST+nH1PPZ75Ql6bhysjKVXhVm+bc2k42f66/VhSEdAAAAADwMTHhf11Nf+iqprq0QU09vOQ7Hc7O07HcAi3/4a9J5QbM/0Y3tKurfpfUM6NUlDJCOgAAAAD4sCCbVd2aR+j1uzrq7XV7VC0wQJHV7Hr8v5slSd/sPKxvdh5WXoFTVj8/ZWSd1IFjubq0YQ1d17Zukfs8me/QwWO5Op5boGaR1eTnx620voKQXg4YhqET+Q7l5DnMLgUAAACASVrWCdW/+7Zyva9bPVgfbNqnZd/vlySN/89Pbu3fXrdHx04WKCPrpDKyTio9K1cH/vz5SE6+q939VzbW4M4NlFfgVFRooHcOBsUipPswwzCUk+fQzSlp2rw/y+xyAAAAAPiQ7i0i1b1FpC4I26L3N+5TrWp2RYUEKsDqpxU/nRoOP+6DH/9xP3M+3a45n26XJD11U2vd2O4CrqybyGIYhmF2Ed6UlZWl0NBQZWZmKiQkxOxyivRP4bxD/epaMjSO2d0BoIIoD31TecM5BVDZjXn/B/3wW6YiQwIVFWpXZLVARYYEKjI0UJEhp8L8d79latCCdYW2rWKzasEdlyi2YU0ZhkHuKAWe9EuEdB9ytnDeonbIn8FcCgrg8WsAUJH4ct9UXnFOAeDc7Dt6QoZhKHXLAU380H24fDW7vwL8/bRkaJwa1apqUoUVgyf9EsPdfYRhGLopJU3rdx9xW346nPNcdAAAAAClrW5YkCRpYFx9xTasoec+3a6P/rzH/VhugZQrXTXjc303oYdCgwPMLLXSIKT7iJw8h1tAJ5wDAAAA8BaLxaLmUSF68qbWurZNHQXb/DXrk1/07Z8Zpc3k/6ldvTDVqGLX3Nsult3fanLFFRch3QcYhqGbU9Jc778dF6+aVWyEcwAAAABeFWzzV4+WUZKkVheEqs2k/7nWbdhzVJLUbvIq9b24rv7d9yIySxkgpJvMMAz9kZ3nuge9Re0QAjoAAAAA04UGBWjViMv14Xe/KyzYpikfnXoue3aeQ29+s0df7/hDwTZ/7c88qUsb1tBz/duZXHHFQEg3kdNp6Jo5a9wmiWPWdgAAAAC+oklkNT3co5kkKa5hTX229YCeWrlVkvTrwWxXu4++36+nb3YoMIBh8OeLkG4Swygc0DvUr65gG3+pAQAAAPieFnVC1KJOiNrXr64vtx1UZEigwoJteuDtjWaXVqEQ0k1yIt/hCugNwqvoo/svY5I4AAAAAD7v0oY1dWnDmpKk47kFruW/HTmhxhE8qu18+ZldAKSP7r9MVez+BHQAAAAA5cqZCeaWF9OKbYdzR0j3AWRzAAAAAOVRFbu/rmldW5J0ODtPB7JOyuk0TK6qfCOkAwAAAABK7PFrW7p+7jg1VTe/mKZdh7JV4HCaWFX5RUj3MsMwlJNXoJw8h9mlAADgNXPnzlVMTIwCAwMVGxurdevWFds2Pz9fkydPVqNGjRQYGKg2bdpoxYoVXqwWAOCJkMAARYUEut6v331EVzy9Wje+sFY//Jap3AKyjycI6V5kGIZuSklTiwkr1eHfn5hdDgAAXrF48WIlJSVp4sSJ2rBhg9q0aaOEhAQdOHCgyPbjxo3Tiy++qDlz5mjz5s0aOnSorr/+em3cyOzBAOCLbP5+Wj3yCn2SdLmqBwe4ln/3W6b6PLdGN6ekMQTeA4R0LzqR79D63UfclnWoX11BPEsQAFCBzZw5U0OGDNHgwYPVokULpaSkKDg4WAsWLCiy/euvv64xY8aoV69eatiwoYYNG6ZevXppxowZXq4cAHCuAgOsahxRTV+PuUqfj7xC4VVtrnXf/5ap8f/50cTqyhcewWaSb8fFK9hmVVAAj10DAFRceXl5Wr9+vUaPHu1a5ufnp/j4eKWlFT0LcG5urgIDA92WBQUFac2aNcV+Tm5urnJzc13vs7KyzrNyAEBJ2P2tql+zir4efZUOHs9VXPKnkqRtGcdNrqz84Eq6SYJtVgXbeOwaAKBiO3TokBwOhyIjI92WR0ZGKj09vchtEhISNHPmTG3btk1Op1OrVq3S0qVLtX///mI/Jzk5WaGhoa5XdHR0qR4HAMAz/lY/1Q4N0tz+7cwupdwhpAMAAJ8ye/ZsNWnSRM2bN5fNZlNiYqIGDx4sP7/if20ZPXq0MjMzXa+9e/d6sWIAQHFOX5Nct+uwPtmcYW4x5QQhHQAAlJnw8HBZrVZlZLj/YpaRkaGoqKgit6lVq5Y++OADZWdna/fu3fr5559VtWpVNWzYsNjPsdvtCgkJcXsBAMxXq5rd9fPdr32rNdsOmVhN+UBIBwAAZcZms6l9+/ZKTU11LXM6nUpNTVVcXNxZtw0MDFTdunVVUFCg9957T9ddd11ZlwsAKGUd6lfXkC4NXO9vn/+Nvtnxh4kV+T5CuhcZPHUAAFAJJSUlad68eXr11Ve1ZcsWDRs2TNnZ2Ro8eLAkaeDAgW4Ty33zzTdaunSpduzYoS+//FI9e/aU0+nUo48+atYhAABKyGKxaPTVF6pfh7/mCun30teFnnqFvzC7u5cYhqGbU4qexRYAgIqsX79+OnjwoCZMmKD09HS1bdtWK1ascE0mt2fPHrf7zU+ePKlx48Zpx44dqlq1qnr16qXXX39dYWFhJh0BAOB8+PlZ9MT1F+l4XoGWfX9qEtB3/m+v2tevbnJlvsliGJXr+m5WVpZCQ0OVmZnp1fvVcvIK1GLCSklSi9ohWvbAZczsDgCQZF7fVJFxTgHA9+Q7nLrphbX67rdMSVLqw13VqFZVk6vyDk/6JYa7m2DJ0DgCOgAAAIBKJcDqp5vOGPb+5td7TKzGdxHSveTM8QrkcwAAAACV0fUX11U1+6m7rvMcDpOr8U2EdC/gfnQAAAAAkKra/XXnZQ3+uWElRkj3ghP5Dm3enyXp1P3oQQFWkysCAAAAAHN99P1+5eQVmF2GzyGkexn3owMAAACozPz+zENHc/L16trdJlfjewjpXsD96AAAAABwSu/WtV0//3E818RKfBMhvYxxPzoAAAAA/KVxRFUN7drI7DJ8FiG9jHE/OgAAAAAUbfvB4zLOHHoM80P63LlzFRMTo8DAQMXGxmrdunVnbT9r1iw1a9ZMQUFBio6O1ogRI3Ty5EkvVXt+uB8dAAAAAP6yeutBvbdhn9ll+BRTQ/rixYuVlJSkiRMnasOGDWrTpo0SEhJ04MCBItu/9dZbGjVqlCZOnKgtW7Zo/vz5Wrx4scaMGePlykuGfA4AAAAAUpcm4a6fdx3KNrES32NqSJ85c6aGDBmiwYMHq0WLFkpJSVFwcLAWLFhQZPu1a9eqc+fO6t+/v2JiYtSjRw/deuut/3j1HQAAAADgOzo3DtcdnWLMLsMnmRbS8/LytH79esXHx/9VjJ+f4uPjlZZW9ERrnTp10vr1612hfMeOHVq+fLl69epV7Ofk5uYqKyvL7QUAAAAAgC/yN+uDDx06JIfDocjISLflkZGR+vnnn4vcpn///jp06JAuu+wyGYahgoICDR069KzD3ZOTkzVp0qRSrR0AAAAAUDqO5xaYXYJPMX3iOE+sXr1aU6dO1fPPP68NGzZo6dKlWrZsmaZMmVLsNqNHj1ZmZqbrtXfvXi9WDAAAAAA4m4Vrd2n11qLnJauMTLuSHh4eLqvVqoyMDLflGRkZioqKKnKb8ePHa8CAAbr77rslSa1atVJ2drbuuecejR07Vn5+hb9zsNvtstvtpX8AAAAAAIASaxMd6vr5x32ZuqJZhInV+A7TrqTbbDa1b99eqamprmVOp1OpqamKi4srcpucnJxCQdxqPfXccZ6tBwAAAADlx/UXX6A+bepIkjKyck2uxneYOtw9KSlJ8+bN06uvvqotW7Zo2LBhys7O1uDBgyVJAwcO1OjRo13t+/TpoxdeeEGLFi3Szp07tWrVKo0fP159+vRxhXUAAAAAQPlQ1X4qx73+9W6GvP/JtOHuktSvXz8dPHhQEyZMUHp6utq2basVK1a4JpPbs2eP25XzcePGyWKxaNy4cdq3b59q1aqlPn366IknnjDrEAAAAAAAJXRpw5p6e92pecO2ZRxnyLski1HJxolnZWUpNDRUmZmZCgkJKfPPy8krUIsJKyVJmycnKNhm6vciAAAf5O2+qTLgnAJA+fHA2xv14Xe/6+qLojS3fzv5+VlkGIZ+zzwpm9VPtaqV/znGPOmXSIwAAAAAANNY/SySpI9/TFf8M58rslqgNu/PUuaJfEnSvy6JVusLwtQ/tp6ZZXpNuXoEW3lUucYpAAAAAIBnrr7or6d77TiYrbQdf7gCuiQt+r+9GvP+D7pr4f9VignDuZJehgzD0M0paWaXAQAAAAA+q0fLKC0ZGqeXvtihumFBalEnRC1qh+jbXYf1n+9+18Y9RyVJqT8f0G9HTii6RrC5BZcxQnoZOpHv0Ob9WZKkFrVDFBTADPQAAAAA8HeXxNTQJTE13JZdVDdUd3RuoJ/Ts9Rz1peSpC5PfqYN47urRhWbGWV6BSHdS5YMjZPFYjG7DAAAAAAoV5pHhahV3VD9sC9TktRuyir965Jo/XrwuLo2raXEK5uYXGHpIqR7CfkcAAAAAEpm8b2X6po5a7TjYLakU/epS9IP+zJ1cb3q+nFfpnYczFbv1rXVvn51VbGX36hbfisHAAAAAFQKwTZ/ffrwFXo2dZv+b9dh1Q0L0qL/26uT+U7d9vI3rnaLvz0V3ptGVtWSezspNDjArJJLjJBehirBxIMAAAAA4DUPXHVqaHtugUOf/3JQ+zNPql6NYAXbrPo5/Zir3S8Zx/XvZZv11M1tzCq1xAjpZYSZ3QEAAACgbNj9rfri0W46ke9QSOCpq+U/7svUwWO5Grzw/yRJS9b/pv6x9XRxvepmluoxnpNeRpjZHQAAAADKToDVzxXQpVOzwXdrHqF5Azu4ll3//Fr9Z9M+M8orMUJ6GTlzqDszuwMAAACAd1zRrJauah7hej/u/R+Vk1cg6dSIZ19HSC8Dfx/qTj4HAAAAAO8IsPrp5UEddGvHepKkY7kFajFhpS6b/qkumrhSX247aHKFZ0dILwMMdQcAAAAA81gsFt17eUO3Zb8dOaHsPIe+3vGHSVWdGyaOK2MMdQcAAAAA74sJr6LvJvTQvC93KCTIX1/vOKxPfz5gdln/iJBexsjnAAAAAGCO0OAAPZLQTJL0+9GTJldzbhjuXgbKwVwEAAAAAFApzf3sV/24L9PsMopFSC9lPB8dAAAAAHxPjSo218/XzFmjtdsPmVhN8QjppYxJ4wAAAADA99zROUbt61d3vR/3nx9118L/0+HsPBOrKoyQXoaYNA4AAAAAfENIYIDeG9ZJ17SuLUnacTBbqT8f0POfbTe5MneE9DJEPgcAAAAA3/LgVU004NL6qlcjWJL08pqd2rjniMlV/YWQDgAAAACoNJpEVtOUvhcpsVtj17Lrn1+rjCzfmP2dkA4AAMrc3LlzFRMTo8DAQMXGxmrdunVnbT9r1iw1a9ZMQUFBio6O1ogRI3TypG/88gQAqBhuuSRavf8c+i5JsVNT9duRHBMrOoWQXsp4/BoAAO4WL16spKQkTZw4URs2bFCbNm2UkJCgAwcOFNn+rbfe0qhRozRx4kRt2bJF8+fP1+LFizVmzBgvVw4AqOjG9rpQYcEBrve/Hsw2sZpTCOmliMevAQBQ2MyZMzVkyBANHjxYLVq0UEpKioKDg7VgwYIi269du1adO3dW//79FRMTox49eujWW2/9x6vvAAB4qk5YkDZN6KHGEVXNLsWFkF6KePwaAADu8vLytH79esXHx7uW+fn5KT4+XmlpRX+x3alTJ61fv94Vynfs2KHly5erV69exX5Obm6usrKy3F4AAJwru7/vRGN/swuoqHj8GgAA0qFDh+RwOBQZGem2PDIyUj///HOR2/Tv31+HDh3SZZddJsMwVFBQoKFDh551uHtycrImTZpUqrUDAGAG3/m6oIIhnwMAUDKrV6/W1KlT9fzzz2vDhg1aunSpli1bpilTphS7zejRo5WZmel67d2714sVAwBQeriSDgAAykx4eLisVqsyMjLclmdkZCgqKqrIbcaPH68BAwbo7rvvliS1atVK2dnZuueeezR27Fj5+RW+xmC322W320v/AAAA8DKupAMAgDJjs9nUvn17paamupY5nU6lpqYqLi6uyG1ycnIKBXGr9dQ8LwaPUQEAlKHME/lml8CV9NLE7w0AABSWlJSkQYMGqUOHDurYsaNmzZql7OxsDR48WJI0cOBA1a1bV8nJyZKkPn36aObMmbr44osVGxur7du3a/z48erTp48rrAMAUBYeeHujYhvUUGRIoGk1ENJLCY9fAwCgaP369dPBgwc1YcIEpaenq23btlqxYoVrMrk9e/a4XTkfN26cLBaLxo0bp3379qlWrVrq06ePnnjiCbMOAQBQwXVsUEM//X7qySC7DmWbGtItRiUbN5aVlaXQ0FBlZmYqJCSk1Pabk1egFhNWSjr1+LVlD1zG7O4AgHNSVn1TZcY5BQB46vInP9OewzlafM+lim1Ys1T37Um/xD3pZYDHrwEAAABA+RJg9Y0MR0gvA+RzAAAAAEBJENIBAAAAAPARhHQAAAAAAHwEIR0AAAAAAB9BSC8llWuOfAAAAABAWSCklwKekQ4AAAAAKA2E9FJwIt+hzftPPfi+Re0QBQVYTa4IAAAAAFAeEdJLGc9IBwAAAACUFCG9lJHPAQAAAAAlRUgHAAAAAMBHENIBAAAAAPARhHQAAAAAAHwEIR0AAAAAAB9BSAcAAAAAwEcQ0gEAAAAA8BGEdAAAAAAAfAQhHQAAAAAAH0FIBwAAAABUesaff3629aCpdRDSAQAAAACVntVikSSlfP6r9vyRY1odhHQAAAAAQKV3R+cY189HcvJMq4OQDgAAAACo9G6Lra+6YUFml0FIBwAAAADAVxDSAQAAAADwEYR0AAAAAAB8BCEdAAAAAAAfQUgHAAAAAMBHmB7S586dq5iYGAUGBio2Nlbr1q07a/ujR49q+PDhql27tux2u5o2barly5d7qVoAAAAAAMqOv5kfvnjxYiUlJSklJUWxsbGaNWuWEhIStHXrVkVERBRqn5eXp+7duysiIkLvvvuu6tatq927dyssLMz7xQMAAAAAUMpMDekzZ87UkCFDNHjwYElSSkqKli1bpgULFmjUqFGF2i9YsECHDx/W2rVrFRAQIEmKiYnxZskAAAAAAJQZ04a75+Xlaf369YqPj/+rGD8/xcfHKy0trchtPvzwQ8XFxWn48OGKjIzURRddpKlTp8rhcBT7Obm5ucrKynJ7AQAAAABQnP2ZJ037bNNC+qFDh+RwOBQZGem2PDIyUunp6UVus2PHDr377rtyOBxavny5xo8frxkzZujf//53sZ+TnJys0NBQ1ys6OrpUj0OSDKPUdwkAAAAA8DLjz3A39I31OpqTZ0oNpk8c5wmn06mIiAi99NJLat++vfr166exY8cqJSWl2G1Gjx6tzMxM12vv3r2lWpNhGLo5pegr/wAAAACA8qNrs7/mRsvIyjWlBtPuSQ8PD5fValVGRobb8oyMDEVFRRW5Te3atRUQECCr1epaduGFFyo9PV15eXmy2WyFtrHb7bLb7aVb/BlO5Du0ef+pIfQtaocoKMD6D1sAAAAAAHxR8g2t9L+f0vVHtjlX0SUTr6TbbDa1b99eqamprmVOp1OpqamKi4srcpvOnTtr+/btcjqdrmW//PKLateuXWRA97YlQ+NksVjMLgMAAAAAUEJmRzpTh7snJSVp3rx5evXVV7VlyxYNGzZM2dnZrtneBw4cqNGjR7vaDxs2TIcPH9aDDz6oX375RcuWLdPUqVM1fPhwsw7Bjdn/MQEAAAAA5Zupj2Dr16+fDh48qAkTJig9PV1t27bVihUrXJPJ7dmzR35+f32PEB0drZUrV2rEiBFq3bq16tatqwcffFCPPfaYWYcAAAAAAECpMTWkS1JiYqISExOLXLd69epCy+Li4vT111+XcVUAAAAAAHhfuZrdHQAAAACAioyQDgAAytzcuXMVExOjwMBAxcbGat26dcW2veKKK2SxWAq9evfu7cWKAQAwh+nD3QEAgG9yOBxauHChUlNTdeDAAbenq0jSp59+ek77Wbx4sZKSkpSSkqLY2FjNmjVLCQkJ2rp1qyIiIgq1X7p0qfLy/nr0zR9//KE2bdro5ptvPr8DAgCgHCCknyfDMLsCAADKxoMPPqiFCxeqd+/euuiii0r8mNGZM2dqyJAhrqe3pKSkaNmyZVqwYIFGjRpVqH2NGjXc3i9atEjBwcGEdABApUBIPw+GYejmlDSzywAAoEwsWrRI77zzjnr16lXifeTl5Wn9+vVuj1T18/NTfHy80tLOrQ+dP3++/vWvf6lKlSrFtsnNzVVubq7rfVZWVolrBgBUbg7nqSuxw9/aoP89dLn8/Lz7rG3uST8PJ/Id2rz/1C8BLWqHKCjAanJFAACUHpvNpsaNG5/XPg4dOiSHw+F6vOppkZGRSk9P/8ft161bpx9//FF33333WdslJycrNDTU9YqOjj6vugEAlVetanZJ0vYDx7X3SI7XP5+QXkqWDI0r8TBAAAB80cMPP6zZs2fLMPHervnz56tVq1bq2LHjWduNHj1amZmZrtfevXu9VCEAoKJ5/rZ2rp/N6AIZ7l5KyOcAgIpmzZo1+uyzz/Txxx+rZcuWCggIcFu/dOnSf9xHeHi4rFarMjIy3JZnZGQoKirqrNtmZ2dr0aJFmjx58j9+jt1ul91u/8d2AAD8k8YR1VTV7q/juQWmfD4hHQAAFCksLEzXX3/9ee3DZrOpffv2Sk1NVd++fSVJTqdTqampSkxMPOu2S5YsUW5urm6//fbzqgEAgPKEkA4AAIr0yiuvlMp+kpKSNGjQIHXo0EEdO3bUrFmzlJ2d7ZrtfeDAgapbt66Sk5Pdtps/f7769u2rmjVrlkodAACUB4R0AABwVgcPHtTWrVslSc2aNVOtWrU82r5fv346ePCgJkyYoPT0dLVt21YrVqxwTSa3Z88e+fm5T5OzdetWrVmzRv/73/9K5yAAACgnCOkAAKBI2dnZuv/++/Xaa6/J6XRKkqxWqwYOHKg5c+YoODj4nPeVmJhY7PD21atXF1rWrFkzUyesAwDALMzuDgAAipSUlKTPP/9c//3vf3X06FEdPXpU//nPf/T555/r4YcfNrs8AAAqJK6kAwCAIr333nt69913dcUVV7iW9erVS0FBQbrlllv0wgsvmFccAAAVFFfSAQBAkXJyclz3jZ8pIiJCOTk5JlQEAEDFR0gHAABFiouL08SJE3Xy5EnXshMnTmjSpEmKi4szsTIAACouhrsDAIAizZ49WwkJCbrgggvUpk0bSdJ3332nwMBArVy50uTqAAComErtSvrSpUvVunXr0todAAAw2UUXXaRt27YpOTlZbdu2Vdu2bTVt2jRt27ZNLVu2NLs8AAAqJI+upL/44otatWqVbDabHnzwQcXGxurTTz/Vww8/rF9++UUDBw4sqzoBAIAJgoODNWTIELPLAACg0jjnkD5t2jRNmDBBrVu31s8//6z//Oc/Gjt2rObMmaMHH3xQ9957r6pXr16WtQIAgDL24Ycf6uqrr1ZAQIA+/PDDs7a99tprvVQVAACVxzmH9FdeeUXz5s3ToEGD9OWXX6pr165au3attm/fripVqpRljQAAwEv69u2r9PR0RUREqG/fvsW2s1gscjgc3isMAIBK4pxD+p49e3TllVdKkrp06aKAgABNmjSJgA4AQAXidDqL/BkAAHjHOU8cl5ubq8DAQNd7m82mGjVqlElRAADANx09etTsEgAAKHN5jlNfVE/7+Gevf7ZHE8eNHz9ewcHBkqS8vDz9+9//VmhoqFubmTNnll51AADANNOnT1dMTIz69esnSbr55pv13nvvqXbt2lq+fLnrsWwAAFQ0NYJtSs86qRU/pevAsZOKqBb4zxuVknMO6Zdffrm2bt3qet+pUyft2LHDrY3FYim9ygAAgKlSUlL05ptvSpJWrVqlTz75RCtWrNA777yjkSNH6n//+5/JFQIAUDZm9muj/vO+kSR5++6vcw7pq1evLsMyAACAr0lPT1d0dLQk6aOPPtItt9yiHj16KCYmRrGxsSZXBwBA2enUKFz+fhYVOA2vf/Y535MuSVlZWVq1apWWLVumgwcPllVNAADAB1SvXl179+6VJK1YsULx8fGSJMMwmNkdAIAycs5X0jdt2qRevXopPT1dklStWjW98847SkhIKLPiAACAeW644Qb1799fTZo00R9//KGrr75akrRx40Y1btzY5OoAAKiYzvlK+mOPPaYGDRroq6++0vr163XVVVcpMTGxLGsDAAAmeuaZZ5SYmKgWLVpo1apVqlq1qiRp//79uu+++0yuDgCAiumcr6SvX79e//vf/9SuXTtJ0oIFC1SjRg1lZWUpJCSkzAoEAADmCAgI0COPPFJo+YgRI0yoBgCAyuGcQ/rhw4d1wQUXuN6HhYWpSpUq+uOPPwjpAABUEB9++KGuvvpqBQQE6MMPPzxr22uvvdZLVQEAUHl49Jz0zZs3u+5Jl05NHLNlyxYdO3bMtax169alVx0AAPCqvn37Kj09XREREerbt2+x7SwWC5PHAQBQBjwK6VdddZUMw30K+muuuUYWi0WGYdBhAwBQzjnPeBis09sPhgUAAOce0nfu3FmWdQAAAAAAUOmdc0h/9dVX9cgjjyg4OLgs6wEAAD7igQceUOPGjfXAAw+4LX/uuee0fft2zZo1y5zCAACowM75EWyTJk3S8ePHy7IWAADgQ9577z117ty50PJOnTrp3XffNaEiAAAqvnMO6X+/Fx0AAFRsf/zxh0JDQwstDwkJ0aFDh0yoCACAiu+cQ7p0aiZXAABQOTRu3FgrVqwotPzjjz9Ww4YNTagIAICKz6PZ3Zs2bfqPQf3w4cPnVRAAAPANSUlJSkxM1MGDB3XllVdKklJTUzVjxgzuRwcAoIx4FNInTZpU5LA3AABQ8dx5553Kzc3VE088oSlTpkiSYmJi9MILL2jgwIEmVwcAQMXkUUj/17/+pYiIiLKqBQAA+Jhhw4Zp2LBhOnjwoIKCglS1alWzSwIAoEI753vSuR8dAIDKp6CgQJ988omWLl3qmkT2999/54kvAACUkXO+ks7s7gAAVC67d+9Wz549tWfPHuXm5qp79+6qVq2apk+frtzcXKWkpJhdIgAAFc45X0l3Op0MdQcAoBJ58MEH1aFDBx05ckRBQUGu5ddff71SU1NNrAwAgIrLo3vSAQBA5fHll19q7dq1stlsbstjYmK0b98+k6oCAKBi8+g56QAAoPJwOp1yOByFlv/222+qVq2aCRUBAFDxEdIBAECRevTo4fY8dIvFouPHj2vixInq1auXeYUBAFCBMdwdAAAU6emnn1bPnj3VokULnTx5Uv3799e2bdsUHh6ut99+2+zyAACokAjpAACgSNHR0fruu++0ePFifffddzp+/Ljuuusu3XbbbW4TyQEAgNJDSAcAAIXk5+erefPm+uijj3TbbbfptttuM7skAAAqBe5JBwAAhQQEBOjkyZNmlwEAQKVDSAcAAEUaPny4pk+froKCArNLAQCg0iCkAwCAIv3f//2fli5dqnr16ikhIUE33HCD28sTc+fOVUxMjAIDAxUbG6t169adtf3Ro0c1fPhw1a5dW3a7XU2bNtXy5cvP53AAACgXuCcdAAAUKSwsTDfeeON572fx4sVKSkpSSkqKYmNjNWvWLCUkJGjr1q2KiIgo1D4vL0/du3dXRESE3n33XdWtW1e7d+9WWFjYedcCAICvI6QDAAA3TqdTTz31lH755Rfl5eXpyiuv1OOPP17iGd1nzpypIUOGaPDgwZKklJQULVu2TAsWLNCoUaMKtV+wYIEOHz6stWvXKiAgQJIUExNT4uMBAKA8Ybg7AABw88QTT2jMmDGqWrWq6tatq2effVbDhw8v0b7y8vK0fv16xcfHu5b5+fkpPj5eaWlpRW7z4YcfKi4uTsOHD1dkZKQuuugiTZ06VQ6Ho9jPyc3NVVZWltsLAIDyiJAOAADcvPbaa3r++ee1cuVKffDBB/rvf/+rN998U06n0+N9HTp0SA6HQ5GRkW7LIyMjlZ6eXuQ2O3bs0LvvviuHw6Hly5dr/PjxmjFjhv79738X+znJyckKDQ11vaKjoz2uFQAAX0BIBwAAbvbs2aNevXq53sfHx8tisej333/3yuc7nU5FRETopZdeUvv27dWvXz+NHTtWKSkpxW4zevRoZWZmul579+71Sq0AAJQ27kkHAABuCgoKFBgY6LYsICBA+fn5Hu8rPDxcVqtVGRkZbsszMjIUFRVV5Da1a9dWQECArFara9mFF16o9PR05eXlyWazFdrGbrfLbrd7XB8AAL7GJ66ke/pYltMWLVoki8Wivn37lm2BAABUIoZh6I477nB73NrJkyc1dOhQjx/BZrPZ1L59e6WmprqWOZ1OpaamKi4urshtOnfurO3bt7sNr//ll19Uu3btIgM6AAAViekh/fRjWSZOnKgNGzaoTZs2SkhI0IEDB8663a5du/TII4+oS5cuXqoUAIDKYdCgQYqIiHC7x/v2229XnTp13Jadq6SkJM2bN0+vvvqqtmzZomHDhik7O9s12/vAgQM1evRoV/thw4bp8OHDevDBB/XLL79o2bJlmjp1aoknrwMAoDwxfbi7p49lkSSHw6HbbrtNkyZN0pdffqmjR496sWIAACq2V155pVT3169fPx08eFATJkxQenq62rZtqxUrVrgmk9uzZ4/8/P66bhAdHa2VK1dqxIgRat26terWrasHH3xQjz32WKnWBQCALzI1pJ9+LMuZ357/02NZJGny5MmKiIjQXXfdpS+//PKsn5Gbm6vc3FzXex7JAgCA9yUmJioxMbHIdatXry60LC4uTl9//XUZVwUAgO8xdbh7SR7LsmbNGs2fP1/z5s07p8/gkSwAAAAAgPLC9HvSPXHs2DENGDBA8+bNU3h4+DltwyNZAAAAAADlhanD3T19LMuvv/6qXbt2qU+fPq5lp2d+9ff319atW9WoUSO3bXgkCwAAAACgvDD1Srqnj2Vp3ry5fvjhB23atMn1uvbaa9WtWzdt2rSJoewAAAAAgHLN9Nndk5KSNGjQIHXo0EEdO3bUrFmzCj2WpW7dukpOTlZgYKAuuugit+3DwsIkqdByAAAAAADKG9NDuqePZQEAAAAAoKIyPaRLnj+W5UwLFy4s/YIAAAAAADABl6gBAAAAAPARhHQAAAAAAHwEIR0AAAAAAB9BSAcAAAAAwEcQ0gEAAAAA8BGEdAAAAAAAfAQhHQAAAAAAH0FIBwAAAADARxDSAQAAAADwEYR0AAAAAAB8BCEdAAAAAAAfQUgHAAAAAMBHENIBAAAAAPARhHQAAAAAAHwEIR0AAAAAAB9BSAcAAAAAwEcQ0gEAAAAA8BGEdAAAAAAAfAQhHQAAAAAAH0FIPw+GYXYFAAAAAICKhJBeQoZh6OaUNLPLAAAAAABUIIT0EjqR79Dm/VmSpBa1QxQUYDW5IgAAAABAeUdILwVLhsbJYrGYXQYAAAAAoJwjpJcC8jkAAAAAoDQQ0gEAAAAA8BGEdAAAAAAAfAQhHQAAAAAAH0FIBwAAAADARxDSAQAAAADwEYR0AAAAAAB8BCEdAAAAAAAfQUgHAAAAAMBHENIBAAAAAPARhHQAAFDm5s6dq5iYGAUGBio2Nlbr1q0rtu3ChQtlsVjcXoGBgV6sFgAA8xDSAQBAmVq8eLGSkpI0ceJEbdiwQW3atFFCQoIOHDhQ7DYhISHav3+/67V7924vVgwAgHkI6QAAoEzNnDlTQ4YM0eDBg9WiRQulpKQoODhYCxYsKHYbi8WiqKgo1ysyMtKLFQMAYB5COgAAKDN5eXlav3694uPjXcv8/PwUHx+vtLS0Yrc7fvy46tevr+joaF133XX66aefzvo5ubm5ysrKcnsBAFAeEdIBAECZOXTokBwOR6Er4ZGRkUpPTy9ym2bNmmnBggX6z3/+ozfeeENOp1OdOnXSb7/9VuznJCcnKzQ01PWKjo4u1eMAAMBbCOkAAMCnxMXFaeDAgWrbtq26du2qpUuXqlatWnrxxReL3Wb06NHKzMx0vfbu3evFigEAKD3+ZhcAAAAqrvDwcFmtVmVkZLgtz8jIUFRU1DntIyAgQBdffLG2b99ebBu73S673X5etQIA4Au4kg4AAMqMzWZT+/btlZqa6lrmdDqVmpqquLi4c9qHw+HQDz/8oNq1a5dVmQAA+AyupAMAgDKVlJSkQYMGqUOHDurYsaNmzZql7OxsDR48WJI0cOBA1a1bV8nJyZKkyZMn69JLL1Xjxo119OhRPfXUU9q9e7fuvvtuMw8DAACvIKQDAIAy1a9fPx08eFATJkxQenq62rZtqxUrVrgmk9uzZ4/8/P4a3HfkyBENGTJE6enpql69utq3b6+1a9eqRYsWZh0CAABeQ0gHAABlLjExUYmJiUWuW716tdv7Z555Rs8884wXqgIAwPdwTzoAAAAAAD6CkA4AAAAAgI8gpAMAAAAA4CMI6QAAAAAA+AhCOgAAAAAAPoKQDgAAAACAjyCkAwAAAADgIwjpAAAAAAD4CEI6AAAAAAA+gpAOAAAAAICPIKQDAAAAAOAjCOkAAAAAAPgIQjoAAAAAAD6CkA4AAAAAgI8gpAMAAAAA4CN8IqTPnTtXMTExCgwMVGxsrNatW1ds23nz5qlLly6qXr26qlevrvj4+LO2BwAAAACgvDA9pC9evFhJSUmaOHGiNmzYoDZt2ighIUEHDhwosv3q1at166236rPPPlNaWpqio6PVo0cP7du3z8uVAwAAAABQukwP6TNnztSQIUM0ePBgtWjRQikpKQoODtaCBQuKbP/mm2/qvvvuU9u2bdW8eXO9/PLLcjqdSk1N9XLlAAAAAACULlNDel5entavX6/4+HjXMj8/P8XHxystLe2c9pGTk6P8/HzVqFGjyPW5ubnKyspyewEAAAAA4ItMDemHDh2Sw+FQZGSk2/LIyEilp6ef0z4ee+wx1alTxy3onyk5OVmhoaGuV3R09HnXDQAAAABAWTB9uPv5mDZtmhYtWqT3339fgYGBRbYZPXq0MjMzXa+9e/d6uUoAAAAAAM6Nv5kfHh4eLqvVqoyMDLflGRkZioqKOuu2Tz/9tKZNm6ZPPvlErVu3Lrad3W6X3W4vlXoBAAAAAChLpl5Jt9lsat++vdukb6cngYuLiyt2uyeffFJTpkzRihUr1KFDB2+UCgAAAABAmTP1SrokJSUladCgQerQoYM6duyoWbNmKTs7W4MHD5YkDRw4UHXr1lVycrIkafr06ZowYYLeeustxcTEuO5dr1q1qqpWrWracQAAAAAAcL5MD+n9+vXTwYMHNWHCBKWnp6tt27ZasWKFazK5PXv2yM/vrwv+L7zwgvLy8nTTTTe57WfixIl6/PHHvVk6AAAAAAClyvSQLkmJiYlKTEwsct3q1avd3u/atavsCwIAAAAAwATlenZ3AAAAAAAqEkI6AAAAAAA+gpAOAAAAAICPIKQDAAAAAOAjCOkAAAAAAPgIQjoAAAAAAD6CkA4AAAAAgI8gpAMAAAAA4CMI6QAAAAAA+AhCOgAAAAAAPoKQDgAAAACAjyCkAwAAAADgIwjpAAAAAAD4CEI6AAAoc3PnzlVMTIwCAwMVGxurdevWndN2ixYtksViUd++fcu2QAAAfAQhHQAAlKnFixcrKSlJEydO1IYNG9SmTRslJCTowIEDZ91u165deuSRR9SlSxcvVQoAgPkI6QAAoEzNnDlTQ4YM0eDBg9WiRQulpKQoODhYCxYsKHYbh8Oh2267TZMmTVLDhg29WC0AAOYipAMAgDKTl5en9evXKz4+3rXMz89P8fHxSktLK3a7yZMnKyIiQnfdddc5fU5ubq6ysrLcXgAAlEeEdAAAUGYOHTokh8OhyMhIt+WRkZFKT08vcps1a9Zo/vz5mjdv3jl/TnJyskJDQ12v6Ojo86obAACzENIBAIDPOHbsmAYMGKB58+YpPDz8nLcbPXq0MjMzXa+9e/eWYZUAAJQdf7MLAAAAFVd4eLisVqsyMjLclmdkZCgqKqpQ+19//VW7du1Snz59XMucTqckyd/fX1u3blWjRo0KbWe322W320u5egAAvI8r6QAAoMzYbDa1b99eqamprmVOp1OpqamKi4sr1L558+b64YcftGnTJtfr2muvVbdu3bRp0yaGsQMAKjyupAMAgDKVlJSkQYMGqUOHDurYsaNmzZql7OxsDR48WJI0cOBA1a1bV8nJyQoMDNRFF13ktn1YWJgkFVoOAEBFREgHAABlql+/fjp48KAmTJig9PR0tW3bVitWrHBNJrdnzx75+TG4DwAAiZAOAAC8IDExUYmJiUWuW7169Vm3XbhwYekXBACAj+JrawAAAAAAfAQhHQAAAAAAH0FIBwAAAADARxDSAQAAAADwEYR0AAAAAAB8BCEdAAAAAAAfQUgHAAAAAMBHENIBAAAAAPARhHQAAAAAAHwEIR0AAAAAAB9BSAcAAAAAwEcQ0gEAAAAA8BGEdAAAAAAAfAQhHQAAAAAAH0FIBwAAAADARxDSAQAAAADwEYR0AAAAAAB8BCEdAAAAAAAfQUgHAAAAAMBHENIBAAAAAPARhHQAAAAAAHwEIR0AAAAAAB9BSAcAAAAAwEcQ0gEAAAAA8BH+ZhcAAJWBYRgqKCiQw+EwuxSYwGq1yt/fXxaLxexSAACAjyOkA0AZy8vL0/79+5WTk2N2KTBRcHCwateuLZvNZnYpAADAhxHSAaAMOZ1O7dy5U1arVXXq1JHNZuNqaiVjGIby8vJ08OBB7dy5U02aNJGfH3ebAQCAohHSAaAM5eXlyel0Kjo6WsHBwWaXA5MEBQUpICBAu3fvVl5engIDA80uCQAA+Ci+ygcAL+DKKfg7AAAAzgW/MQAAAAAA4CMI6QAAAAAA+AhCOgAAAAAAPoKQDgA4q7S0NFmtVvXu3dtt+erVq2WxWHT06NFC28TExGjWrFluyz777DP16tVLNWvWVHBwsFq0aKGHH35Y+/btK3Ftc+fOVUxMjAIDAxUbG6t169adtX1+fr4mT56sRo0aKTAwUG3atNGKFSvc2jgcDo0fP14NGjRQUFCQGjVqpClTpsgwjCL3OXToUFkslkLHCwAAUBKEdADAWc2fP1/333+/vvjiC/3+++8l2seLL76o+Ph4RUVF6b333tPmzZuVkpKizMxMzZgxo0T7XLx4sZKSkjRx4kRt2LBBbdq0UUJCgg4cOFDsNuPGjdOLL76oOXPmaPPmzRo6dKiuv/56bdy40dVm+vTpeuGFF/Tcc89py5Ytmj59up588knNmTOn0P7ef/99ff3116pTp06JjgEAAODveAQbAHiZYRg6ke8w5bODAqwePaf9+PHjWrx4sb799lulp6dr4cKFGjNmjEef+dtvv+mBBx7QAw88oGeeeca1PCYmRpdffnmRV+LPxcyZMzVkyBANHjxYkpSSkqJly5ZpwYIFGjVqVJHbvP766xo7dqx69eolSRo2bJg++eQTzZgxQ2+88YYkae3atbruuutcIwdiYmL09ttvF7pKv2/fPt1///1auXJloVEGAAAAJeUTIX3u3Ll66qmnlJ6erjZt2mjOnDnq2LFjse2XLFmi8ePHa9euXWrSpImmT5/u+oULAHzdiXyHWkxYacpnb56coGDbuf/T/84776h58+Zq1qyZbr/9dj300EMaPXq0R0F/yZIlysvL06OPPlrk+rCwMEnSnj171KJFi7Pua8yYMRozZozy8vK0fv16jR492rXOz89P8fHxSktLK3b73NzcQs8oDwoK0po1a1zvO3XqpJdeekm//PKLmjZtqu+++05r1qzRzJkzXW2cTqcGDBigkSNHqmXLlmetGQAAwBOmh/TTwxVTUlIUGxurWbNmKSEhQVu3blVERESh9mvXrtWtt96q5ORkXXPNNXrrrbfUt29fbdiwQRdddJEJRwAAFdf8+fN1++23S5J69uypzMxMff7557riiivOeR/btm1TSEiIateufdZ2derU0aZNm87apkaNGpKkQ4cOyeFwKDIy0m19ZGSkfv7552K3T0hI0MyZM3X55ZerUaNGSk1N1dKlS+Vw/DWyYdSoUcrKylLz5s1ltVrlcDj0xBNP6LbbbnO1mT59uvz9/fXAAw+ctV4AAABPmR7SPR2uOHv2bPXs2VMjR46UJE2ZMkWrVq3Sc889p5SUFK/WDgAlERRg1ebJCaZ99rnaunWr1q1bp/fff1+S5O/vr379+mn+/PkehXTDMM7pyru/v78aN258zvstidmzZ2vIkCFq3ry5LBaLGjVqpMGDB2vBggWuNu+8847efPNNvfXWW2rZsqU2bdqkhx56SHXq1NGgQYO0fv16zZ49Wxs2bPBoRAEAAMC5MDWkl2S4YlpampKSktyWJSQk6IMPPiiyfW5urnJzc13vs7Kyzr9wADgPFovFoyHnZpk/f74KCgrcJkUzDEN2u13PPfecQkJCJEmZmZmuIeunHT16VKGhoZKkpk2bKjMzU/v37z/r1XRPhruHh4fLarUqIyPDbX1GRoaioqKK3b5WrVr64IMPdPLkSf3xxx+qU6eORo0apYYNG7rajBw5UqNGjdK//vUvSVKrVq20e/duJScna9CgQfryyy914MAB1atXz7WNw+HQww8/rFmzZmnXrl1nPQYAAICzMfW3xJIMV0xPTy+yfXp6epHtk5OTNWnSpNIpGAAqiYKCAr322muaMWOGevTo4baub9++evvtt3XbbbfJz89P69evV/369V3rd+zYoczMTDVt2lSSdNNNN2nUqFF68skn3SaOO+3o0aMKCwvzaLi7zWZT+/btlZqaqr59+0o6dZ94amqqEhMT//H4AgMDVbduXeXn5+u9997TLbfc4lqXk5MjPz/3h59YrVY5nU5J0oABAxQfH++2PiEhQQMGDHCNCgMAACgp37+Uc55Gjx7tduU9KytL0dHR573fM4erejJ8FADKg48++khHjhzRXXfd5boiftqNN96o+fPna+jQobr77rv18MMPy9/fX61atdLevXv12GOP6dJLL1WnTp0kSdHR0XrmmWeUmJiorKwsDRw4UDExMfrtt9/02muvqWrVqpoxY4bHw92TkpI0aNAgdejQQR07dtSsWbOUnZ3tFpQHDhyounXrKjk5WZL0zTffaN++fWrbtq327dunxx9/XE6n021Suz59+uiJJ55QvXr11LJlS23cuFEzZ87UnXfeKUmqWbOmatas6VZLQECAoqKi1KxZM89OdCXiySSxS5cu1dSpU7V9+3bl5+erSZMmevjhhzVgwAAvVw0AqMxevbOjDEMKCw7w6ueaGtJLMlwxKirKo/Z2u112u710Cj5DeRmuCgAlMX/+fMXHxxcK6NKpkP7kk0/q+++/1+zZszVt2jQ99thj2r17t6KiotS9e3c98cQTbvdr33fffWratKmefvppXX/99Tpx4oRiYmJ0zTXXFLqF6Vz169dPBw8e1IQJE5Senq62bdtqxYoVbqOt9uzZ43ZV/OTJkxo3bpx27NihqlWrqlevXnr99dfdhuvPmTNH48eP13333acDBw6oTp06uvfeezVhwoQS1QnPJ4mtUaOGxo4dq+bNm8tms+mjjz7S4MGDFRERoYQEc+ZzAABUPp0bh5vyuRbDMAxTPvlPsbGx6tixo+bMmSPp1HDFevXqKTExsciJ4/r166ecnBz997//dS3r1KmTWrdufU4Tx2VlZSk0NFSZmZmu+ykBoKycPHlSO3fuVIMGDQo9+guVy9n+LlT0vik2NlaXXHKJnnvuOUmn+vro6Gjdf//9xT7T/u/atWun3r17a8qUKefUvqKfUwBA+eJJv+R31rVekJSUpHnz5unVV1/Vli1bNGzYMLfhigMHDnSbWO7BBx/UihUrNGPGDP388896/PHH9e23357TPYgAAMC7Tk8Se+Z9/OfyTPvTDMNQamqqtm7dqssvv7zYdrm5ucrKynJ7AQBQHpk+Xvufhiv+fahip06d9NZbb2ncuHEaM2aMmjRpog8++IBnpAMA4INK+kz7zMxM1a1bV7m5ubJarXr++efVvXv3YtszUSwAoKIwPaRLUmJiYrFXwlevXl1o2c0336ybb765jKsCAABmqVatmjZt2qTjx48rNTVVSUlJatiwoa644ooi25fVRLEAAHibT4R0AABQMZX0mfZ+fn6u2f7btm2rLVu2KDk5udiQXlYTxQIA4G2m35MOAJWByXN0wgdU1r8DZz7T/rTTz7SPi4s75/04nU7l5uaWRYkAAPgUrqQDQBkKCDj1XM2cnBwFBQWZXA3MlJOTI+mvvxOVyT890/7vz7NPTk5Whw4d1KhRI+Xm5mr58uV6/fXX9cILL5h5GAAAeAUhHQDKkNVqVVhYmA4cOCBJCg4Odnt+OCo+wzCUk5OjAwcOKCwsTFar1eySvM7TSWKzs7N133336bffflNQUJCaN2+uN954Q/369TPrEAAA8BrTn5PubTw3FYC3GYah9PR0HT161OxSYKKwsDBFRUUV+SUNfVPp45wCAHyJJ/0SV9IBoIxZLBbVrl1bERERys/PN7scmCAgIKBSXkEHAACeI6QDgJdYrVaCGgAAAM6K2d0BAAAAAPARhHQAAAAAAHwEIR0AAAAAAB9R6e5JPz2ZfVZWlsmVAABwyuk+qZI9cKVM0d8DAHyJJ319pQvpx44dkyRFR0ebXAkAAO6OHTum0NBQs8uoEOjvAQC+6Fz6+kr3nHSn06nff/9d1apVK/JZtZ7IyspSdHS09u7dyzNYzxHnzHOcM89xzjzHOfNcaZ4zwzB07Ngx1alTR35+3IlWGujvzcU58wzny3OcM89xzjxnVl9f6a6k+/n56YILLijVfYaEhPAX3UOcM89xzjzHOfMc58xzpXXOuIJeuujvfQPnzDOcL89xzjzHOfOct/t6vq4HAAAAAMBHENIBAAAAAPARhPTzYLfbNXHiRNntdrNLKTc4Z57jnHmOc+Y5zpnnOGeVB/+tPcc58wzny3OcM89xzjxn1jmrdBPHAQAAAADgq7iSDgAAAACAjyCkAwAAAADgIwjpAAAAAAD4CEI6AAAAAAA+gpD+D+bOnauYmBgFBgYqNjZW69atO2v7JUuWqHnz5goMDFSrVq20fPlyL1XqOzw5Z/PmzVOXLl1UvXp1Va9eXfHx8f94jisiT/+enbZo0SJZLBb17du3bAv0QZ6es6NHj2r48OGqXbu27Ha7mjZtWun+//T0nM2aNUvNmjVTUFCQoqOjNWLECJ08edJL1Zrriy++UJ8+fVSnTh1ZLBZ98MEH/7jN6tWr1a5dO9ntdjVu3FgLFy4s8zpReujvPUd/7xn6es/R13uOvt4zPtvfGyjWokWLDJvNZixYsMD46aefjCFDhhhhYWFGRkZGke2/+uorw2q1Gk8++aSxefNmY9y4cUZAQIDxww8/eLly83h6zvr372/MnTvX2Lhxo7FlyxbjjjvuMEJDQ43ffvvNy5Wbx9NzdtrOnTuNunXrGl26dDGuu+467xTrIzw9Z7m5uUaHDh2MXr16GWvWrDF27txprF692ti0aZOXKzePp+fszTffNOx2u/Hmm28aO3fuNFauXGnUrl3bGDFihJcrN8fy5cuNsWPHGkuXLjUkGe+///5Z2+/YscMIDg42kpKSjM2bNxtz5swxrFarsWLFCu8UjPNCf+85+nvP0Nd7jr7ec/T1nvPV/p6QfhYdO3Y0hg8f7nrvcDiMOnXqGMnJyUW2v+WWW4zevXu7LYuNjTXuvffeMq3Tl3h6zv6uoKDAqFatmvHqq6+WVYk+pyTnrKCgwOjUqZPx8ssvG4MGDap0Hben5+yFF14wGjZsaOTl5XmrRJ/j6TkbPny4ceWVV7otS0pKMjp37lymdfqic+m0H330UaNly5Zuy/r162ckJCSUYWUoLfT3nqO/9wx9vefo6z1HX39+fKm/Z7h7MfLy8rR+/XrFx8e7lvn5+Sk+Pl5paWlFbpOWlubWXpISEhKKbV/RlOSc/V1OTo7y8/NVo0aNsirTp5T0nE2ePFkRERG66667vFGmTynJOfvwww8VFxen4cOHKzIyUhdddJGmTp0qh8PhrbJNVZJz1qlTJ61fv941TG7Hjh1avny5evXq5ZWay5vK/u9/eUZ/7zn6e8/Q13uOvt5z9PXe4a1///1LdW8VyKFDh+RwOBQZGem2PDIyUj///HOR26SnpxfZPj09vczq9CUlOWd/99hjj6lOnTqF/vJXVCU5Z2vWrNH8+fO1adMmL1Toe0pyznbs2KFPP/1Ut912m5YvX67t27frvvvuU35+viZOnOiNsk1VknPWv39/HTp0SJdddpkMw1BBQYGGDh2qMWPGeKPkcqe4f/+zsrJ04sQJBQUFmVQZ/gn9vefo7z1DX+85+nrP0dd7h7f6e66kw2dMmzZNixYt0vvvv6/AwECzy/FJx44d04ABAzRv3jyFh4ebXU654XQ6FRERoZdeeknt27dXv379NHbsWKWkpJhdms9avXq1pk6dqueff14bNmzQ0qVLtWzZMk2ZMsXs0gCUc/T3Z0dfXzL09Z6jr/ddXEkvRnh4uKxWqzIyMtyWZ2RkKCoqqshtoqKiPGpf0ZTknJ329NNPa9q0afrkk0/UunXrsizTp3h6zn799Vft2rVLffr0cS1zOp2SJH9/f23dulWNGjUq26JNVpK/Z7Vr11ZAQICsVqtr2YUXXqj09HTl5eXJZrOVac1mK8k5Gz9+vAYMGKC7775bktSqVStlZ2frnnvu0dixY+Xnx3e8Zyru3/+QkBCuovs4+nvP0d97hr7ec/T1nqOv9w5v9fec+WLYbDa1b99eqamprmVOp1OpqamKi4srcpu4uDi39pK0atWqYttXNCU5Z5L05JNPasqUKVqxYoU6dOjgjVJ9hqfnrHnz5vrhhx+0adMm1+vaa69Vt27dtGnTJkVHR3uzfFOU5O9Z586dtX37dtcvOZL0yy+/qHbt2hW+05ZKds5ycnIKdc6nf/E5NbcKzlTZ//0vz+jvPUd/7xn6es/R13uOvt47vPbvf6lOQ1fBLFq0yLDb7cbChQuNzZs3G/fcc48RFhZmpKenG4ZhGAMGDDBGjRrlav/VV18Z/v7+xtNPP21s2bLFmDhxYqV8JIsn52zatGmGzWYz3n33XWP//v2u17Fjx8w6BK/z9Jz9XWWc8dXTc7Znzx6jWrVqRmJiorF161bjo48+MiIiIox///vfZh2C13l6ziZOnGhUq1bNePvtt40dO3YY//vf/4xGjRoZt9xyi1mH4FXHjh0zNm7caGzcuNGQZMycOdPYuHGjsXv3bsMwDGPUqFHGgAEDXO1PP5Jl5MiRxpYtW4y5c+fyCLZyhP7ec/T3nqGv9xx9vefo6z3nq/09If0fzJkzx6hXr55hs9mMjh07Gl9//bVrXdeuXY1Bgwa5tX/nnXeMpk2bGjabzWjZsqWxbNkyL1dsPk/OWf369Q1JhV4TJ070fuEm8vTv2ZkqY8dtGJ6fs7Vr1xqxsbGG3W43GjZsaDzxxBNGQUGBl6s2lyfnLD8/33j88ceNRo0aGYGBgUZ0dLRx3333GUeOHPF+4Sb47LPPivy36fQ5GjRokNG1a9dC27Rt29aw2WxGw4YNjVdeecXrdaPk6O89R3/vGfp6z9HXe46+3jO+2t9bDIOxDAAAAAAA+ALuSQcAAAAAwEcQ0gEAAAAA8BGEdAAAAAAAfAQhHQAAAAAAH0FIBwAAAADARxDSAQAAAADwEYR0AAAAAAB8BCEdAAAAAAAfQUgHAAAAUOosFos++OADSdKuXbtksVi0adMmU2sCygNCOgA3d9xxhywWS6HX9u3b3dbZbDY1btxYkydPVkFBgSRp9erVbtvUqlVLvXr10g8//GDyUQEAULmc2WcHBASoQYMGevTRR3Xy5EmzSwPwDwjpAArp2bOn9u/f7/Zq0KCB27pt27bp4Ycf1uOPP66nnnrKbfutW7dq//79WrlypXJzc9W7d2/l5eWZcSgAAFRap/vsHTt26JlnntGLL76oiRMnml0WgH9ASAdQiN1uV1RUlNvLarW6ratfv76GDRum+Ph4ffjhh27bR0REKCoqSu3atdNDDz2kvXv36ueffzbjUAAAqLRO99nR0dHq27ev4uPjtWrVKkmS0+lUcnKyGjRooKCgILVp00bvvvuu2/Y//fSTrrnmGoWEhKhatWrq0qWLfv31V0nS//3f/6l79+4KDw9XaGiounbtqg0bNnj9GIGKiJAO4LwEBQUVe5U8MzNTixYtkiTZbDZvlgUAAM7w448/au3ata7+ODk5Wa+99ppSUlL0008/acSIEbr99tv1+eefS5L27dunyy+/XHa7XZ9++qnWr1+vO++803WL27FjxzRo0CCtWbNGX3/9tZo0aaJevXrp2LFjph0jUFH4m10AAN/z0UcfqWrVqq73V199tZYsWeLWxjAMpaamauXKlbr//vvd1l1wwQWSpOzsbEnStddeq+bNm5dx1QAA4Eyn+/OCggLl5ubKz89Pzz33nHJzczV16lR98skniouLkyQ1bNhQa9as0YsvvqiuXbtq7ty5Cg0N1aJFixQQECBJatq0qWvfV155pdtnvfTSSwoLC9Pnn3+ua665xnsHCVRAhHQAhXTr1k0vvPCC632VKlVcP5/u8PPz8+V0OtW/f389/vjjbtt/+eWXCg4O1tdff62pU6cqJSXFW6UDAIA/ne7Ps7Oz9cwzz8jf31833nijfvrpJ+Xk5Kh79+5u7fPy8nTxxRdLkjZt2qQuXbq4AvrfZWRkaNy4cVq9erUOHDggh8OhnJwc7dmzp8yPC6joCOkACqlSpYoaN25c5LrTHb7NZlOdOnXk71/4n5EGDRooLCxMzZo104EDB9SvXz998cUXZV02AAA4w5n9+YIFC9SmTRvNnz9fF110kSRp2bJlqlu3rts2drtd0qnb2c5m0KBB+uOPPzR79mzVr19fdrtdcXFxTBQLlALuSQfgkdMdfr169YoM6H83fPhw/fjjj3r//fe9UB0AACiKn5+fxowZo3HjxqlFixay2+3as2ePGjdu7PaKjo6WJLVu3Vpffvml8vPzi9zfV199pQceeEC9evVSy5YtZbfbdejQIW8eElBhEdIBlKng4GANGTJEEydOlGEYZpcDAECldfPNN8tqterFF1/UI488ohEjRujVV1/Vr7/+qg0bNmjOnDl69dVXJUmJiYnKysrSv/71L3377bfatm2bXn/9dW3dulWS1KRJE73++uvasmWLvvnmG912223/ePUdwLkhpAMoc4mJidqyZUuhyecAAID3+Pv7KzExUU8++aRGjx6t8ePHKzk5WRdeeKF69uypZcuWqUGDBpKkmjVr6tNPP9Xx48fVtWtXtW/fXvPmzXPdoz5//nwdOXJE7dq104ABA/TAAw8oIiLCzMMDKgyLwaUtAAAAAAB8AlfSAQAAAADwEYR0AAAAAAB8BCEdAAAAAAAfQUgHAAAAAMBHENIBAAAAAPARhHQAAAAAAHwEIR0AAAAAAB9BSAcAAAAAwEcQ0gEAAAAA8BGEdAAAAAAAfAQhHQAAAAAAH/H/vmwAIDPE5nsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EVENT-LEVEL METRICS ===\n",
            "Event TP: 262 FP: 2 FN: 141\n",
            "Event Precision: 0.9924 Event Recall: 0.6501 Event F1: 0.7856\n",
            "Saved event-level JSON -> /content/drive/MyDrive/AccidentProject/eval/event_results.json\n",
            "Saved human summary -> /content/drive/MyDrive/AccidentProject/eval/eval_summary.txt\n",
            "\n",
            "Done. CSV and JSON results are in: /content/drive/MyDrive/AccidentProject/eval\n"
          ]
        }
      ]
    }
  ]
}